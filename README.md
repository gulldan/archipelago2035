## Архипелаг 2035 — SAR/UAV Object Detection

### решение
- Что делаем: находим людей на больших снимках с дронов (в т.ч. SAR) быстро и надёжно.
- Как работает: сначала обзор всего кадра (1280×1280) — ищем «подозрительные зоны», затем проверяем их точнее тайлами 1536×1536.
- Почему быстро: модель в формате ONNX, запуск через ONNX Runtime на GPU; тайлы обрабатываются батчами; зависимостей минимум.
- Что сдаём: один небольшой файл кода (`submission.py`) и файл модели (`best.onnx`). В проде не нужен PyTorch/Ultralytics/OpenCV.
- Как проверяется: есть ноутбук с метрикой организаторов — подключаем нашу `predict`, строим `prediction.csv` и считаем оценку.
- Если «железо» слабое: автоматически подстраиваем размер батча под VRAM; есть запасной режим на CPU.

### Краткое резюме (Executive summary)
- Цель: надёжное обнаружение людей на больших UAV/SAR изображениях при ограничении VRAM/времени.
- Ключевая идея: двухэтапный инференс (coarse 1280² → точные тайлы 1536²), батчинг, NMS, всё на ONNX Runtime GPU.
- Доставка: минимальный сабмит (`submission.py` + `best.onnx`), без PyTorch/Ultralytics/OpenCV.
- Результат: простой и быстрый пайплайн, совместимый с требованиями организатора к формату и времени.

### Идея и цель
Поиск людей на аэрофотоснимках (SAR/UAV) с ограничениями по времени и ресурсам. Цель: получить устойчивое качество на больших изображениях (до 8k×8k) при ограничении VRAM и времени инференса. Подход: тайловый инференс с упором на лучшую утилизацию GPU, минимальный runtime и простую доставку в виде ONNX + onnxruntime-gpu.

### Датасеты
- VisDrone: масштабный UAV-бенчмарк (разные города/дроны/условия), >2.6M аннотированных объектов — использован для дообучения/регуляризации и устойчивости к разнообразным сценам.
- LaDD (Large-scale Aerial Drone Dataset): добавлен для расширения домена (углы, масштабы, плотность объектов), улучшает обобщение на низкоконтрастных/мелких целях.

### Разделение данных
- Train/Val/Test: 70% / 15% / 15% по изображениям (стратификация по локациям/съёмочным сессиям, где возможно), исключая «протечки» между сплитами.
- Валидация фиксирована (без утечек из train); тест — только для финальной оценки.

### Базовый EDA
- Распределение классов и дисбаланс (люди/пустые кадры). Нормализуем веса/семплинг на train.
- Размеры объектов: доля мелких целей (<16–24 px) — выбор `tile/stride`, аугментаций и NMS.
- Условия съёмки: разное освещение, шум/смаз — добавлены деградации/фотометрия. EDA в `eda-v2.ipynb`.

Данные по `ground_truth.csv` (быстрый EDA):
- **Изображений**: 340; **всего боксов**: 865; **среднее боксов/изображение**: 2.544 (медиана 2)
- **Ширина бокса (норм.)**: mean 0.01718, median 0.01513, p25 0.01105, p75 0.02153, p90 0.02944, p95 0.03252
- **Высота бокса (норм.)**: mean 0.02437, median 0.02155, p25 0.01767, p75 0.02802, p90 0.04234, p95 0.04720
- **Мелкие объекты (<2% по ширине и высоте)**: 268 (30.98%) — обосновывает тайлы 1024–1536 и аккуратный NMS

### Особенности и преимущества
- Три обучающих конвейера в репозитории:
  - `train_yolo11_v4.py`: YOLOv11 (Ultralytics), тайловый train/val, SAHI-подобный инференс. Базовая и понятная версия для быстрых экспериментов (грид-серчи инференса, метрика организаторов).
  - `train_yolo11_v6.py`: доработанный пайплайн под финальную фазу (фиксированный размер трансформа, UAV-аугментации, «final phase» на последних эпохах, аккуратные якоря), H100-friendly. Упор на воспроизводимость и стабильность качества на высоких разрешениях.
  - `train_frcnn_uav_v2.py`: Итеративный датасет и ускоренная инфраструктура (IterableDataset, LRU-кэш изображений, TurboJPEG, CUDA prefetcher). Используется для оценки альтернатив и валидации решений в условиях ограничений по I/O и памяти.

- Подготовка и EDA:
  - `prepare_yolo_data.py` — скрипт подготовки размеченных данных в формат YOLO с контролем качества и фильтрацией аномалий.
  - EDA нотбуки (`eda-v2.ipynb`) — анализ распределений по размеру объектов, плотности целей, профили времени; помогают выбирать параметры тайлинга (`tile/stride/overlap`) и аугментаций.

- Инференс (submission):
  - `solution/yolo11m-v4-1536-25e-new-sol/submission.py` — минималистичный, без логов и профилирования, на чистом `onnxruntime-gpu` (или CPU fallback). Поддержка батч-обработки тайлов, IO binding (если доступен), NMS на NumPy. Зависимости сведены к минимуму: `numpy`, `onnx`, `onnxruntime-gpu`, `Pillow`.
  - Поддержка FP16 ONNX: если рядом с `submission.py` есть `best.fp16.onnx`, он будет выбран автоматически.

### Состав сдачи
- `solution/yolo11m-v4-1536-25e-new-sol/submission.py` — точка входа: `predict(images)`.
- `solution/yolo11m-v4-1536-25e-new-sol/best.onnx` — модель (поддержка `best.fp16.onnx`).
- Для обучения/переобучения (при необходимости): `export.py` (экспорт `.pt→.onnx`), `train_*.py` (скрипты обучения), `metric_counter.ipynb` (валидация метрикой организаторов).

### Архитектура / Реализация
- Модельные пайплайны
  - `train_yolo11_v4.py` (YOLOv11 «v4»):
    - `SARTileDataset` режет исходные изображения на сетку тайлов; «позитив» — центр GT в тайле или покрытие плиткой ≥ `pos_iou_thr/cover_thr`.
    - Аугментации: `LongestMaxSize`+`PadIfNeeded` до `imgsz`, геометрия (Rotate/Affine), фотометрия, деградации; bbox-aware (`Albumentations`).
    - Train: Ultralytics YOLO, гиперпараметры подобраны под высокие разрешения (скоринг box/cls/dfl, SGD/AdamW). Val: только позитивные тайлы.
    - Инференс: SAHI-подобный тайлинг (`slice_size`, `overlap`), fuse через WBF/NMS, грид-серч порогов с метрикой организаторов.

  - `train_yolo11_v6.py` (YOLOv11 «v6» финализ.):
    - Фиксированная форма трансформа (compile/benchmark-friendly), UAV-аугментации, «final phase» в конце обучения (ослабление аугментов для стабилизации).
    - Тщательная настройка якорей/порогов, паттерн обучения на высоких `imgsz` (TF32/AMP, ограничение воркеров на hi-res, косинус LR).
    - Повышенная воспроизводимость, стабильность детекций мелких объектов и robustness к сдвигам распределения.

  - `train_frcnn_uav_v2.py` (Faster R-CNN baselines):
    - `IterableDataset` c LRU-кэшем целых изображений (worker-local), TurboJPEG для быстрых JPG, Pillow-SIMD на fallback.
    - Тайловая выборка на лету, DDP- и worker-шардинг, CUDA prefetcher, AMP (`bf16/fp16`), фиксированный transform size в `torchvision`.
    - RPN/ROI конфигурация под мелкие цели (якоря 4..64, повышенный top‑N, батч в ROI), NMS 0.5.

- Инференс:
  - Большие кадры режутся на тайлы (`TILE_SIZE=1536`, `OVERLAP≈0.15`), батч-обработка тайлов (автокалибровка по VRAM), ORT run, преобразование координат → глобальные, финальный NMS.
  - Минимальный runtime: без OpenCV (Pillow BILINEAR), без Ultralytics/PyTorch в проде — только `onnxruntime-gpu`.

### Соответствие требованиям хакатона
- Формат задачи и ожидания организатора изложены на странице задания — см. [описание задачи](https://buildin.ai/share/4af5d8c2-898e-45f4-ab31-8737a9ef2269?code=C017S4).
- Формат предсказаний: проект выдаёт CSV/список детекций с полями `image_id, label, xc, yc, w, h, w_img, h_img, score, time_spent`, совместимыми с проверяющим скриптом организаторов (`metric.py`, `metric_counter.ipynb`).
- Точка входа сабмита: `submission.predict(images)` принимает одно изображение (`np.ndarray HxWx3 uint8 RGB`) или список и возвращает список детекций в нормированных координатах — соответствует интеграции в проверяющую среду.
- Ограничения по времени/ресурсам: используется `onnxruntime-gpu` с автоподбором размера батча под VRAM (RTX 4090 12 GB), CPU fallback включён. Зависимости сведены к минимуму для лёгкого контейнера.
- Репродукция метрики: `metric_counter.ipynb` подключает `predict` из `submission.py` и формирует `prediction.csv`, далее оценивается метрика организаторов по IoU-гриду (см. `metric.py`).

### Описание `solution.py` и `submission.py`
- `solution.py` (полная версия с логированием):
  - Выбор EP: опциональный TensorRT (через `ENABLE_TRT=1`), иначе CUDA→CPU fallback.
  - Батч‑инференс тайлов через ORT, попытка I/O binding (`OrtValue`) на GPU; при ошибке — обычный `run`.
  - Двухуровневый пайплайн: coarse‑обзор на 1280² (поиск регионов интереса) → точные тайлы `1536×1536` с NMS и фильтрацией по размеру.
  - Тайловая сетка/перекрытие, финальный NMS на всех детекциях кадра.
- `submission.py` (минимум зависимостей, без логов): тот же алгоритм тайлинга/батчинга/NMS, Pillow вместо OpenCV, простая инициализация ORT, авто‑выбор `best.fp16.onnx` при наличии.

### Описание `prepare_yolo_data.py`
- Собирает YOLO‑датасет из исходных папок (`Rescue‑SAR`, `LaDD`, др.):
  - Стратифицированный сплит train/val (по pos/neg), опционально группировкой по сценам; гарант мин. позитивов в val.
  - Чинит разметку: кламп в [0,1], фильтрация нулевых боксов, сведение классов к одному (person=0).
  - Пишет `mapping.csv` и `dataset.yaml` с агрегированной статистикой.

### Ссылка на описание задачи
- Краткие детали задачи и мотивация решения: [страница описания](https://buildin.ai/share/4af5d8c2-898e-45f4-ab31-8737a9ef2269?code=C017S4)

### Как проверить за 2 минуты
1) Проверка точки входа:
```python
import numpy as np
from solution.yolo11m-v4-1536-25e-new-sol.submission import predict
img = np.zeros((1536,1536,3), dtype=np.uint8)
print(predict([img])[0])
```
2) Посчитать метрику (через ноутбук): открыть `metric_counter.ipynb`, запустить ячейки — ноутбук сам импортирует `predict`, построит `prediction.csv` и выведет оценку.

### Пользовательский сценарий
1) Обучение (пример для `train_yolo11_v6.py`):
```bash
uv run python train_yolo11_v6.py train \
  --data ./dataset/yolo_dataset/dataset.yaml \
  --model l \
  --epochs 60 \
  --batch 16 \
  --imgsz 1792 \
  --tile-train 1 --tile 1024 --stride 512 \
  --pos-frac 0.9 --max-neg 4 --workers 16 \
  --cover-thr 0.30 --final-epochs 10 --rand-phase 1
```

2) Экспорт модели в ONNX:
```bash
uv run python export.py --pt solution/yolo11m-v4-1536-25e-new-sol/best.pt --onnx solution/yolo11m-v4-1536-25e-new-sol/best.onnx --imgsz 1536 --opset 12 --fp16 0
```

3) Локальная проверка инференса submission:
```python
import numpy as np
from solution.yolo11m-v4-1536-25e-new-sol.submission import predict

img = np.zeros((1536,1536,3), dtype=np.uint8)
dets = predict([img])
print(len(dets[0]))
```

4) Минимальные зависимости для контейнера инференса:
`onnx`, `onnxruntime-gpu`, `numpy`, `Pillow`. Базовый образ: `nvidia/cuda:12.x-cudnn-runtime-ubuntu22.04`.

5) Оценка метрики (локально):
В `metric_counter.ipynb` подключена функция `predict` из нашего submission, формируется `prediction.csv`, далее оценивается метрика организаторов (`metric.py`).

### Презентация решения (для защиты)
- Проблема и ограничения: большие кадры, много мелких целей, лимиты по VRAM/времени → тайловая обработка, GPU‑батчинг, минимальный runtime.
- Данные: собственный SAR/UAV + VisDrone + LaDD; честный сплит 70/15/15; EDA показал ~31% очень мелких объектов → выбраны тайлы 1024–1536 и аккуратный NMS.
- Модель/архитектура: YOLOv11 пайплайны (`v4`, `v6`) и альтернативный Faster R‑CNN для валидации идей (мелкие якоря, фикс‑размер трансформа, UAV‑аугментации, final‑phase).
- Инференс: двухэтапный (coarse→fine), ORT GPU, I/O binding (когда доступен), автокалибровка батча под 12 GB, NMS один раз на весь кадр.
- Инфраструктура: `uv` для зависимостей, `export.py` для `.pt→.onnx`, минимальные зависимости (`onnx`, `onnxruntime-gpu`, `numpy`, `Pillow`).
- Репродукция/оценка: `metric_counter.ipynb` + `metric.py`, формат CSV организаторов.
- Ограничения и риски: мелкие богатые сцены могут требовать более глубокий TTA/кастомные якоря; TensorRT возможен опционально (включается `ENABLE_TRT=1`).
- Дальнейшая работа: FP16/INT8 ONNX, стабильный TRT‑engine cache, более агрессивный батчинг, улучшение coarse‑локатора.

### Источники/ссылки
- Описание задачи: [buildin.ai](https://buildin.ai/share/4af5d8c2-898e-45f4-ab31-8737a9ef2269?code=C017S4)
- Про VisDrone: [Ultralytics Docs](https://docs.ultralytics.com/ru/datasets/detect/visdrone)
---
Вопросы/идеи по улучшению: TensorRT EP (опционально), FP16/INT8 ONNX, ещё более агрессивный батчинг с IO binding.

# archipelago2035
техзрение.архипелаг2035.рф
