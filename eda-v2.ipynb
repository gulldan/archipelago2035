{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO\n",
    "- Lacmus Drone Dataset (LaDD) https://habr.com/ru/companies/ods/articles/483616/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# –ò–º–ø–æ—Ä—Ç—ã\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from PIL import Image\n",
    "import json\n",
    "from typing import Optional, Union\n",
    "\n",
    "from tqdm import tqdm\n",
    "import polars as pl\n",
    "from PIL import ImageDraw\n",
    "import plotly.express as px\n",
    "\n",
    "\n",
    "# –ù–∞—Å—Ç—Ä–æ–π–∫–∏ –¥–ª—è –≥—Ä–∞—Ñ–∏–∫–æ–≤\n",
    "plt.style.use(\"default\")\n",
    "sns.set_palette(\"husl\")\n",
    "plt.rcParams[\"figure.figsize\"] = (12, 8)\n",
    "plt.rcParams[\"font.size\"] = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ø—É—Ç–µ–π –∫ –¥–∞–Ω–Ω—ã–º\n",
    "BASE_PATH = Path(\"./dataset\")  # –ò–∑–º–µ–Ω–∏—Ç–µ –Ω–∞ –≤–∞—à –ø—É—Ç—å\n",
    "\n",
    "# –ü—É—Ç–∏ –∫ –¥–∞—Ç–∞—Å–µ—Ç–∞–º\n",
    "datasets = {\n",
    "    \"train_s1\": BASE_PATH / \"01_train-s1__DataSet_Human_Rescue\",\n",
    "    \"train_s2\": BASE_PATH / \"02_second_part_DataSet_Human_Rescue\",\n",
    "    \"validation_public\": BASE_PATH / \"03_validation__DataSet_Human_Rescue/public\",\n",
    "    \"validation_private\": BASE_PATH / \"03_validation__DataSet_Human_Rescue/private\",\n",
    "}\n",
    "\n",
    "# –ü—Ä–æ–≤–µ—Ä—è–µ–º –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç—å –¥–∞–Ω–Ω—ã—Ö\n",
    "print(\"üîç –ü—Ä–æ–≤–µ—Ä–∫–∞ –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç–∏ –¥–∞—Ç–∞—Å–µ—Ç–æ–≤:\")\n",
    "available_datasets = {}\n",
    "for name, path in datasets.items():\n",
    "    if path.exists():\n",
    "        available_datasets[name] = path\n",
    "        print(f\"‚úÖ {name}: {path}\")\n",
    "    else:\n",
    "        print(f\"‚ùå {name}: –Ω–µ –Ω–∞–π–¥–µ–Ω - {path}\")\n",
    "\n",
    "print(f\"\\nüìä –î–æ—Å—Ç—É–ø–Ω–æ –¥–∞—Ç–∞—Å–µ—Ç–æ–≤: {len(available_datasets)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def extract_dataset_to_parquet(\n",
    "    datasets_path: Union[str, Path],\n",
    "    output_path: Union[str, Path],\n",
    "    sample_size: Optional[int] = None,\n",
    "    include_image_stats: bool = True,\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    –ò–∑–≤–ª–µ–∫–∞–µ—Ç –ø–æ–ª–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –¥–∞—Ç–∞—Å–µ—Ç–µ –∏ —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç –≤ parquet —Ñ–∞–π–ª\n",
    "\n",
    "    –ò—â–µ—Ç –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –∏ –∞–Ω–Ω–æ—Ç–∞—Ü–∏–∏ —Ç–æ–ª—å–∫–æ –≤ —Å–ª–µ–¥—É—é—â–∏—Ö –ø–∞–ø–∫–∞—Ö:\n",
    "    - 01_train-s1__DataSet_Human_Rescue (—Å–æ—Ö—Ä–∞–Ω—è–µ—Ç—Å—è –∫–∞–∫ train_s1)\n",
    "    - 02_second_part_DataSet_Human_Rescue (—Å–æ—Ö—Ä–∞–Ω—è–µ—Ç—Å—è –∫–∞–∫ train_s2)\n",
    "    - 03_validation__DataSet_Human_Rescue (—Å–æ—Ö—Ä–∞–Ω—è–µ—Ç—Å—è –∫–∞–∫ validation)\n",
    "\n",
    "    Args:\n",
    "        datasets_path: –ü—É—Ç—å –∫ –ø–∞–ø–∫–µ —Å–æ–¥–µ—Ä–∂–∞—â–µ–π –ø–∞–ø–∫–∏ –¥–∞—Ç–∞—Å–µ—Ç–æ–≤ (–Ω–∞–ø—Ä–∏–º–µ—Ä '/mnt/data/dataset')\n",
    "        output_path: –ü—É—Ç—å –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è parquet —Ñ–∞–π–ª–∞\n",
    "        sample_size: –û–≥—Ä–∞–Ω–∏—á–∏—Ç—å –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ (None = –≤—Å–µ)\n",
    "        include_image_stats: –í–∫–ª—é—á–∞—Ç—å –ª–∏ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π (—Ä–∞–∑–º–µ—Ä —Ñ–∞–π–ª–∞ –∏ —Ç.–¥.)\n",
    "\n",
    "    Returns:\n",
    "        –ü—É—Ç—å –∫ —Å–æ–∑–¥–∞–Ω–Ω–æ–º—É parquet —Ñ–∞–π–ª—É\n",
    "    \"\"\"\n",
    "\n",
    "    datasets_path = Path(datasets_path)\n",
    "    output_path = Path(output_path)\n",
    "\n",
    "    if not datasets_path.exists():\n",
    "        raise ValueError(f\"–ü—É—Ç—å –∫ –¥–∞—Ç–∞—Å–µ—Ç–∞–º –Ω–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç: {datasets_path}\")\n",
    "\n",
    "    output_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    print(f\"üîç –°–∫–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ –¥–∞—Ç–∞—Å–µ—Ç–æ–≤ –≤ {datasets_path}\")\n",
    "\n",
    "    # –°–ø–∏—Å–æ–∫ –¥–ª—è —Ö—Ä–∞–Ω–µ–Ω–∏—è –≤—Å–µ—Ö –¥–∞–Ω–Ω—ã—Ö\n",
    "    all_data = []\n",
    "\n",
    "    # –ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã–µ —Ñ–æ—Ä–º–∞—Ç—ã –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π\n",
    "    image_extensions = [\".jpg\", \".jpeg\", \".png\", \".JPG\", \".JPEG\", \".PNG\"]\n",
    "\n",
    "    # –û–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω—ã–µ –ø–∞–ø–∫–∏ –¥–ª—è –ø–æ–∏—Å–∫–∞\n",
    "    target_datasets = [\n",
    "        \"01_train-s1__DataSet_Human_Rescue\",\n",
    "        \"02_second_part_DataSet_Human_Rescue\",\n",
    "        \"03_validation__DataSet_Human_Rescue\",\n",
    "    ]\n",
    "\n",
    "    # –ù–∞—Ö–æ–¥–∏–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è —Ç–æ–ª—å–∫–æ –≤ —É–∫–∞–∑–∞–Ω–Ω—ã—Ö –ø–∞–ø–∫–∞—Ö –¥–∞—Ç–∞—Å–µ—Ç–æ–≤\n",
    "    all_images = []\n",
    "    for dataset_name in target_datasets:\n",
    "        dataset_path = datasets_path / dataset_name\n",
    "        if dataset_path.exists():\n",
    "            print(f\"üìÅ –°–∫–∞–Ω–∏—Ä—É—é {dataset_name}...\")\n",
    "            for ext in image_extensions:\n",
    "                found_images = list(dataset_path.rglob(f\"*{ext}\"))\n",
    "                all_images.extend(found_images)\n",
    "                if found_images:\n",
    "                    print(f\"   –ù–∞–π–¥–µ–Ω–æ {len(found_images)} —Ñ–∞–π–ª–æ–≤ {ext}\")\n",
    "        else:\n",
    "            print(f\"‚ö†Ô∏è –ü–∞–ø–∫–∞ {dataset_name} –Ω–µ –Ω–∞–π–¥–µ–Ω–∞\")\n",
    "\n",
    "    if sample_size and len(all_images) > sample_size:\n",
    "        all_images = np.random.choice(all_images, sample_size, replace=False).tolist()\n",
    "\n",
    "    print(f\"üìä –ù–∞–π–¥–µ–Ω–æ {len(all_images)} –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞\")\n",
    "\n",
    "    # –°–æ–∑–¥–∞–µ–º —Å–ª–æ–≤–∞—Ä—å –¥–ª—è –±—ã—Å—Ç—Ä–æ–≥–æ –ø–æ–∏—Å–∫–∞ –∞–Ω–Ω–æ—Ç–∞—Ü–∏–π –≤ —Ç–µ—Ö –∂–µ –ø–∞–ø–∫–∞—Ö\n",
    "    print(\"üîç –ü–æ–∏—Å–∫ –∞–Ω–Ω–æ—Ç–∞—Ü–∏–π...\")\n",
    "    all_labels = []\n",
    "    for dataset_name in target_datasets:\n",
    "        dataset_path = datasets_path / dataset_name\n",
    "        if dataset_path.exists():\n",
    "            labels_in_dataset = list(dataset_path.rglob(\"*.txt\"))\n",
    "            all_labels.extend(labels_in_dataset)\n",
    "\n",
    "    labels_dict = {label.stem: label for label in all_labels}\n",
    "\n",
    "    print(f\"üìù –ù–∞–π–¥–µ–Ω–æ {len(all_labels)} —Ñ–∞–π–ª–æ–≤ –∞–Ω–Ω–æ—Ç–∞—Ü–∏–π\")\n",
    "\n",
    "    # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –∫–∞–∂–¥–æ–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ\n",
    "    for img_path in tqdm(all_images, desc=\"–û–±—Ä–∞–±–æ—Ç–∫–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π\"):\n",
    "        try:\n",
    "            # –ë–∞–∑–æ–≤–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ–± –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–∏\n",
    "            img_info = {\n",
    "                \"image_path\": str(img_path),\n",
    "                \"image_name\": img_path.name,\n",
    "                \"image_stem\": img_path.stem,\n",
    "                \"dataset_name\": _get_dataset_name(img_path, datasets_path),\n",
    "                \"relative_path\": str(img_path.relative_to(datasets_path)),\n",
    "            }\n",
    "\n",
    "            # –ó–∞–≥—Ä—É–∂–∞–µ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è —Ä–∞–∑–º–µ—Ä–æ–≤\n",
    "            try:\n",
    "                with Image.open(img_path) as img:\n",
    "                    img_width, img_height = img.size\n",
    "                    img_format = img.format\n",
    "                    img_mode = img.mode\n",
    "\n",
    "                img_info.update(\n",
    "                    {\n",
    "                        \"image_width\": img_width,\n",
    "                        \"image_height\": img_height,\n",
    "                        \"image_format\": img_format,\n",
    "                        \"image_mode\": img_mode,\n",
    "                        \"image_aspect_ratio\": img_width / img_height,\n",
    "                        \"image_total_pixels\": img_width * img_height,\n",
    "                    }\n",
    "                )\n",
    "\n",
    "                if include_image_stats:\n",
    "                    img_info.update(\n",
    "                        {\n",
    "                            \"image_size_bytes\": img_path.stat().st_size,\n",
    "                            \"image_size_mb\": img_path.stat().st_size / (1024 * 1024),\n",
    "                        }\n",
    "                    )\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ {img_path}: {e}\")\n",
    "                continue\n",
    "\n",
    "            # –ò—â–µ–º —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â—É—é –∞–Ω–Ω–æ—Ç–∞—Ü–∏—é\n",
    "            annotation_path = labels_dict.get(img_path.stem)\n",
    "\n",
    "            if annotation_path and annotation_path.exists():\n",
    "                img_info[\"annotation_path\"] = str(annotation_path)\n",
    "                img_info[\"has_annotation\"] = True\n",
    "\n",
    "                # –ü–∞—Ä—Å–∏–º –∞–Ω–Ω–æ—Ç–∞—Ü–∏–∏\n",
    "                try:\n",
    "                    with open(annotation_path, \"r\") as f:\n",
    "                        lines = [line.strip() for line in f.readlines() if line.strip()]\n",
    "\n",
    "                    img_info[\"objects_count\"] = len(lines)\n",
    "\n",
    "                    if len(lines) > 0:\n",
    "                        # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –∫–∞–∂–¥—ã–π –æ–±—ä–µ–∫—Ç\n",
    "                        for obj_idx, line in enumerate(lines):\n",
    "                            parts = line.split()\n",
    "                            if len(parts) >= 5:\n",
    "                                class_id = int(parts[0])\n",
    "                                x_center = float(parts[1])\n",
    "                                y_center = float(parts[2])\n",
    "                                width_norm = float(parts[3])\n",
    "                                height_norm = float(parts[4])\n",
    "\n",
    "                                # –í—ã—á–∏—Å–ª—è–µ–º –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç—ã –≤ –ø–∏–∫—Å–µ–ª—è—Ö\n",
    "                                width_pix = width_norm * img_width\n",
    "                                height_pix = height_norm * img_height\n",
    "                                x_center_pix = x_center * img_width\n",
    "                                y_center_pix = y_center * img_height\n",
    "\n",
    "                                # –ö–æ–æ—Ä–¥–∏–Ω–∞—Ç—ã —É–≥–ª–æ–≤\n",
    "                                x1 = x_center_pix - width_pix / 2\n",
    "                                y1 = y_center_pix - height_pix / 2\n",
    "                                x2 = x_center_pix + width_pix / 2\n",
    "                                y2 = y_center_pix + height_pix / 2\n",
    "\n",
    "                                # –ü–ª–æ—â–∞–¥—å –∏ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏\n",
    "                                area_pix = width_pix * height_pix\n",
    "                                area_percent = (\n",
    "                                    area_pix / (img_width * img_height)\n",
    "                                ) * 100\n",
    "                                bbox_aspect_ratio = (\n",
    "                                    width_pix / height_pix if height_pix > 0 else 0\n",
    "                                )\n",
    "\n",
    "                                # –°–æ–∑–¥–∞–µ–º –∑–∞–ø–∏—Å—å –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –æ–±—ä–µ–∫—Ç–∞\n",
    "                                obj_data = img_info.copy()\n",
    "                                obj_data.update(\n",
    "                                    {\n",
    "                                        \"object_id\": obj_idx,\n",
    "                                        \"class_id\": class_id,\n",
    "                                        \"x_center_norm\": x_center,\n",
    "                                        \"y_center_norm\": y_center,\n",
    "                                        \"width_norm\": width_norm,\n",
    "                                        \"height_norm\": height_norm,\n",
    "                                        \"x_center_pix\": x_center_pix,\n",
    "                                        \"y_center_pix\": y_center_pix,\n",
    "                                        \"width_pix\": width_pix,\n",
    "                                        \"height_pix\": height_pix,\n",
    "                                        \"x1\": x1,\n",
    "                                        \"y1\": y1,\n",
    "                                        \"x2\": x2,\n",
    "                                        \"y2\": y2,\n",
    "                                        \"bbox_area_pix\": area_pix,\n",
    "                                        \"bbox_area_percent\": area_percent,\n",
    "                                        \"bbox_aspect_ratio\": bbox_aspect_ratio,\n",
    "                                    }\n",
    "                                )\n",
    "\n",
    "                                all_data.append(obj_data)\n",
    "                    else:\n",
    "                        # –ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ –±–µ–∑ –æ–±—ä–µ–∫—Ç–æ–≤\n",
    "                        img_info[\"objects_count\"] = 0\n",
    "                        img_info[\"object_id\"] = None\n",
    "                        all_data.append(img_info)\n",
    "\n",
    "                except Exception as e:\n",
    "                    print(f\"‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–∞—Ä—Å–∏–Ω–≥–µ –∞–Ω–Ω–æ—Ç–∞—Ü–∏–∏ {annotation_path}: {e}\")\n",
    "                    img_info[\"objects_count\"] = 0\n",
    "                    img_info[\"annotation_error\"] = str(e)\n",
    "                    all_data.append(img_info)\n",
    "            else:\n",
    "                # –ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ –±–µ–∑ –∞–Ω–Ω–æ—Ç–∞—Ü–∏–∏\n",
    "                img_info[\"annotation_path\"] = None\n",
    "                img_info[\"has_annotation\"] = False\n",
    "                img_info[\"objects_count\"] = 0\n",
    "                img_info[\"object_id\"] = None\n",
    "                all_data.append(img_info)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è –û–±—â–∞—è –æ—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ {img_path}: {e}\")\n",
    "            continue\n",
    "\n",
    "    # –°–æ–∑–¥–∞–µ–º DataFrame\n",
    "    print(\"üìä –°–æ–∑–¥–∞–Ω–∏–µ DataFrame...\")\n",
    "    df = pd.DataFrame(all_data)\n",
    "\n",
    "    # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ –ø—É—Ç–∏ –∫ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—é –∏ id –æ–±—ä–µ–∫—Ç–∞\n",
    "    df = df.sort_values([\"image_path\", \"object_id\"]).reset_index(drop=True)\n",
    "\n",
    "    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ parquet\n",
    "    parquet_path = output_path\n",
    "    if not str(parquet_path).endswith(\".parquet\"):\n",
    "        parquet_path = output_path / \"dataset_info.parquet\"\n",
    "\n",
    "    print(f\"üíæ –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≤ {parquet_path}\")\n",
    "    df.to_parquet(parquet_path, index=False)\n",
    "\n",
    "    # –°–æ–∑–¥–∞–µ–º summary\n",
    "    summary = {\n",
    "        \"total_images\": len(df[\"image_path\"].unique()),\n",
    "        \"total_objects\": len(df[df[\"object_id\"].notna()]),\n",
    "        \"datasets\": list(df[\"dataset_name\"].unique()),\n",
    "        \"image_formats\": list(df[\"image_format\"].unique()),\n",
    "        \"has_annotations\": int(df[\"has_annotation\"].sum()),\n",
    "        \"images_without_annotations\": int((~df[\"has_annotation\"]).sum()),\n",
    "        \"columns\": list(df.columns),\n",
    "        \"file_size_mb\": parquet_path.stat().st_size / (1024 * 1024),\n",
    "    }\n",
    "\n",
    "    # –°–æ—Ö—Ä–∞–Ω—è–µ–º summary\n",
    "    summary_path = parquet_path.parent / f\"{parquet_path.stem}_summary.json\"\n",
    "    with open(summary_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(summary, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "    print(\"\\n‚úÖ –ì–æ—Ç–æ–≤–æ!\")\n",
    "    print(f\"üìÅ Parquet —Ñ–∞–π–ª: {parquet_path}\")\n",
    "    print(f\"üìã Summary: {summary_path}\")\n",
    "    print(\n",
    "        f\"üìä –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ: {summary['total_images']} –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π, {summary['total_objects']} –æ–±—ä–µ–∫—Ç–æ–≤\"\n",
    "    )\n",
    "    print(f\"üíæ –†–∞–∑–º–µ—Ä —Ñ–∞–π–ª–∞: {summary['file_size_mb']:.2f} MB\")\n",
    "\n",
    "    return str(parquet_path)\n",
    "\n",
    "\n",
    "def _get_dataset_name(img_path: Path, base_path: Path) -> str:\n",
    "    \"\"\"–û–ø—Ä–µ–¥–µ–ª—è–µ—Ç –Ω–∞–∑–≤–∞–Ω–∏–µ –¥–∞—Ç–∞—Å–µ—Ç–∞ –ø–æ –ø—É—Ç–∏ —Ñ–∞–π–ª–∞\"\"\"\n",
    "    try:\n",
    "        relative_path = img_path.relative_to(base_path)\n",
    "        # –ë–µ—Ä–µ–º –ø–µ—Ä–≤—É—é –ø–∞–ø–∫—É –≤ –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω–æ–º –ø—É—Ç–∏\n",
    "        parts = relative_path.parts\n",
    "        if len(parts) > 0:\n",
    "            dataset_folder = parts[0]\n",
    "            # –°–æ–∫—Ä–∞—â–∞–µ–º –¥–ª–∏–Ω–Ω—ã–µ –Ω–∞–∑–≤–∞–Ω–∏—è –¥–ª—è —É–¥–æ–±—Å—Ç–≤–∞\n",
    "            if dataset_folder == \"01_train-s1__DataSet_Human_Rescue\":\n",
    "                return \"train_s1\"\n",
    "            elif dataset_folder == \"02_second_part_DataSet_Human_Rescue\":\n",
    "                return \"train_s2\"\n",
    "            elif dataset_folder == \"03_validation__DataSet_Human_Rescue\":\n",
    "                return \"validation\"\n",
    "            else:\n",
    "                return dataset_folder\n",
    "        else:\n",
    "            return \"unknown\"\n",
    "    except:\n",
    "        return \"unknown\"\n",
    "\n",
    "\n",
    "def load_dataset_info(parquet_path: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    –ó–∞–≥—Ä—É–∂–∞–µ—Ç –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –¥–∞—Ç–∞—Å–µ—Ç–µ –∏–∑ parquet —Ñ–∞–π–ª–∞\n",
    "\n",
    "    Args:\n",
    "        parquet_path: –ü—É—Ç—å –∫ parquet —Ñ–∞–π–ª—É\n",
    "\n",
    "    Returns:\n",
    "        DataFrame —Å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–µ–π –æ –¥–∞—Ç–∞—Å–µ—Ç–µ\n",
    "    \"\"\"\n",
    "    return pd.read_parquet(parquet_path)\n",
    "\n",
    "\n",
    "def get_dataset_summary(df: pd.DataFrame) -> dict:\n",
    "    \"\"\"\n",
    "    –°–æ–∑–¥–∞–µ—Ç –∫—Ä–∞—Ç–∫—É—é —Å–≤–æ–¥–∫—É –ø–æ –¥–∞—Ç–∞—Å–µ—Ç—É\n",
    "\n",
    "    Args:\n",
    "        df: DataFrame —Å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–µ–π –æ –¥–∞—Ç–∞—Å–µ—Ç–µ\n",
    "\n",
    "    Returns:\n",
    "        –°–ª–æ–≤–∞—Ä—å —Å–æ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–æ–π\n",
    "    \"\"\"\n",
    "    summary = {\n",
    "        \"total_images\": len(df[\"image_path\"].unique()),\n",
    "        \"total_objects\": len(df[df[\"object_id\"].notna()]),\n",
    "        \"avg_objects_per_image\": df.groupby(\"image_path\")[\"object_id\"].count().mean(),\n",
    "        \"datasets\": df[\"dataset_name\"].value_counts().to_dict(),\n",
    "        \"image_size_stats\": {\n",
    "            \"width\": {\n",
    "                \"mean\": df[\"image_width\"].mean(),\n",
    "                \"std\": df[\"image_width\"].std(),\n",
    "                \"min\": df[\"image_width\"].min(),\n",
    "                \"max\": df[\"image_width\"].max(),\n",
    "            },\n",
    "            \"height\": {\n",
    "                \"mean\": df[\"image_height\"].mean(),\n",
    "                \"std\": df[\"image_height\"].std(),\n",
    "                \"min\": df[\"image_height\"].min(),\n",
    "                \"max\": df[\"image_height\"].max(),\n",
    "            },\n",
    "        },\n",
    "        \"bbox_size_stats\": {\n",
    "            \"area_percent\": {\n",
    "                \"mean\": df[df[\"object_id\"].notna()][\"bbox_area_percent\"].mean(),\n",
    "                \"std\": df[df[\"object_id\"].notna()][\"bbox_area_percent\"].std(),\n",
    "                \"min\": df[df[\"object_id\"].notna()][\"bbox_area_percent\"].min(),\n",
    "                \"max\": df[df[\"object_id\"].notna()][\"bbox_area_percent\"].max(),\n",
    "            }\n",
    "        },\n",
    "    }\n",
    "\n",
    "    return summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hashlib\n",
    "from pathlib import Path\n",
    "from typing import Union, Optional\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def _file_md5(path: Union[str, Path], chunk_size: int = 1024 * 1024) -> Optional[str]:\n",
    "    \"\"\"\n",
    "    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç MD5-—Ö–µ—à —Ñ–∞–π–ª–∞ –ø–æ –µ–≥–æ –ø—É—Ç–∏. –ß—Ç–µ–Ω–∏–µ –∏–¥—ë—Ç –±–ª–æ–∫–∞–º–∏, —á—Ç–æ–±—ã –Ω–µ –∑–∞–≥—Ä—É–∂–∞—Ç—å\n",
    "    –≤–µ—Å—å —Ñ–∞–π–ª –≤ –ø–∞–º—è—Ç—å. –í —Å–ª—É—á–∞–µ –æ—à–∏–±–∫–∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç None.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        md5 = hashlib.md5()\n",
    "        with open(path, \"rb\") as f:\n",
    "            while True:\n",
    "                chunk = f.read(chunk_size)\n",
    "                if not chunk:\n",
    "                    break\n",
    "                md5.update(chunk)\n",
    "        return md5.hexdigest()\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "\n",
    "def extract_dataset_to_parquet(\n",
    "    datasets_path: Union[str, Path],\n",
    "    output_path: Union[str, Path],\n",
    "    sample_size: Optional[int] = None,\n",
    "    include_image_stats: bool = True,\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    –ò–∑–≤–ª–µ–∫–∞–µ—Ç –ø–æ–ª–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –¥–∞—Ç–∞—Å–µ—Ç–µ –∏ —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç –≤ parquet —Ñ–∞–π–ª\n",
    "\n",
    "    –ò—â–µ—Ç –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –∏ –∞–Ω–Ω–æ—Ç–∞—Ü–∏–∏ —Ç–æ–ª—å–∫–æ –≤ —Å–ª–µ–¥—É—é—â–∏—Ö –ø–∞–ø–∫–∞—Ö:\n",
    "    - 01_train-s1__DataSet_Human_Rescue (—Å–æ—Ö—Ä–∞–Ω—è–µ—Ç—Å—è –∫–∞–∫ train_s1)\n",
    "    - 02_second_part_DataSet_Human_Rescue (—Å–æ—Ö—Ä–∞–Ω—è–µ—Ç—Å—è –∫–∞–∫ train_s2)\n",
    "    - 03_validation__DataSet_Human_Rescue (—Å–æ—Ö—Ä–∞–Ω—è–µ—Ç—Å—è –∫–∞–∫ validation)\n",
    "\n",
    "    –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è —Ä–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ—Ç—Å—è MD5-—Ö–µ—à —Ñ–∞–π–ª–∞ (–∫–æ–ª–æ–Ω–∫–∞ `md5`).\n",
    "\n",
    "    Args:\n",
    "        datasets_path: –ü—É—Ç—å –∫ –ø–∞–ø–∫–µ —Å–æ–¥–µ—Ä–∂–∞—â–µ–π –ø–∞–ø–∫–∏ –¥–∞—Ç–∞—Å–µ—Ç–æ–≤ (–Ω–∞–ø—Ä–∏–º–µ—Ä '/mnt/data/dataset')\n",
    "        output_path: –ü—É—Ç—å –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è parquet —Ñ–∞–π–ª–∞\n",
    "        sample_size: –û–≥—Ä–∞–Ω–∏—á–∏—Ç—å –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ (None = –≤—Å–µ)\n",
    "        include_image_stats: –í–∫–ª—é—á–∞—Ç—å –ª–∏ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π (—Ä–∞–∑–º–µ—Ä —Ñ–∞–π–ª–∞ –∏ —Ç.–¥.)\n",
    "\n",
    "    Returns:\n",
    "        –ü—É—Ç—å –∫ —Å–æ–∑–¥–∞–Ω–Ω–æ–º—É parquet —Ñ–∞–π–ª—É\n",
    "    \"\"\"\n",
    "\n",
    "    datasets_path = Path(datasets_path)\n",
    "    output_path = Path(output_path)\n",
    "\n",
    "    if not datasets_path.exists():\n",
    "        raise ValueError(f\"–ü—É—Ç—å –∫ –¥–∞—Ç–∞—Å–µ—Ç–∞–º –Ω–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç: {datasets_path}\")\n",
    "\n",
    "    output_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    print(f\"üîç –°–∫–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ –¥–∞—Ç–∞—Å–µ—Ç–æ–≤ –≤ {datasets_path}\")\n",
    "\n",
    "    # –°–ø–∏—Å–æ–∫ –¥–ª—è —Ö—Ä–∞–Ω–µ–Ω–∏—è –≤—Å–µ—Ö –¥–∞–Ω–Ω—ã—Ö\n",
    "    all_data = []\n",
    "\n",
    "    # –ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã–µ —Ñ–æ—Ä–º–∞—Ç—ã –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π\n",
    "    image_extensions = [\".jpg\", \".jpeg\", \".png\", \".JPG\", \".JPEG\", \".PNG\"]\n",
    "\n",
    "    # –û–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω—ã–µ –ø–∞–ø–∫–∏ –¥–ª—è –ø–æ–∏—Å–∫–∞\n",
    "    target_datasets = [\n",
    "        \"01_train-s1__DataSet_Human_Rescue\",\n",
    "        \"02_second_part_DataSet_Human_Rescue\",\n",
    "        \"03_validation__DataSet_Human_Rescue\",\n",
    "        \"04_ladd\",\n",
    "        \"05_pd\",\n",
    "        \"06_dataset_yolo9\",\n",
    "    ]\n",
    "\n",
    "    # –ù–∞—Ö–æ–¥–∏–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è —Ç–æ–ª—å–∫–æ –≤ —É–∫–∞–∑–∞–Ω–Ω—ã—Ö –ø–∞–ø–∫–∞—Ö –¥–∞—Ç–∞—Å–µ—Ç–æ–≤\n",
    "    all_images = []\n",
    "    for dataset_name in target_datasets:\n",
    "        dataset_path = datasets_path / dataset_name\n",
    "        if dataset_path.exists():\n",
    "            print(f\"üìÅ –°–∫–∞–Ω–∏—Ä—É—é {dataset_name}...\")\n",
    "            for ext in image_extensions:\n",
    "                found_images = list(dataset_path.rglob(f\"*{ext}\"))\n",
    "                all_images.extend(found_images)\n",
    "                if found_images:\n",
    "                    print(f\"   –ù–∞–π–¥–µ–Ω–æ {len(found_images)} —Ñ–∞–π–ª–æ–≤ {ext}\")\n",
    "        else:\n",
    "            print(f\"‚ö†Ô∏è –ü–∞–ø–∫–∞ {dataset_name} –Ω–µ –Ω–∞–π–¥–µ–Ω–∞\")\n",
    "\n",
    "    if sample_size and len(all_images) > sample_size:\n",
    "        all_images = np.random.choice(all_images, sample_size, replace=False).tolist()\n",
    "\n",
    "    print(f\"üìä –ù–∞–π–¥–µ–Ω–æ {len(all_images)} –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞\")\n",
    "\n",
    "    # –°–æ–∑–¥–∞–µ–º —Å–ª–æ–≤–∞—Ä—å –¥–ª—è –±—ã—Å—Ç—Ä–æ–≥–æ –ø–æ–∏—Å–∫–∞ –∞–Ω–Ω–æ—Ç–∞—Ü–∏–π –≤ —Ç–µ—Ö –∂–µ –ø–∞–ø–∫–∞—Ö\n",
    "    print(\"üîç –ü–æ–∏—Å–∫ –∞–Ω–Ω–æ—Ç–∞—Ü–∏–π...\")\n",
    "    all_labels = []\n",
    "    for dataset_name in target_datasets:\n",
    "        dataset_path = datasets_path / dataset_name\n",
    "        if dataset_path.exists():\n",
    "            labels_in_dataset = list(dataset_path.rglob(\"*.txt\"))\n",
    "            all_labels.extend(labels_in_dataset)\n",
    "\n",
    "    labels_dict = {label.stem: label for label in all_labels}\n",
    "\n",
    "    print(f\"üìù –ù–∞–π–¥–µ–Ω–æ {len(all_labels)} —Ñ–∞–π–ª–æ–≤ –∞–Ω–Ω–æ—Ç–∞—Ü–∏–π\")\n",
    "\n",
    "    # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –∫–∞–∂–¥–æ–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ\n",
    "    for img_path in tqdm(all_images, desc=\"–û–±—Ä–∞–±–æ—Ç–∫–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π\"):\n",
    "        try:\n",
    "            # MD5-—Ö–µ—à —Ñ–∞–π–ª–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è\n",
    "            md5_hash = _file_md5(img_path)\n",
    "\n",
    "            # –ë–∞–∑–æ–≤–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ–± –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–∏\n",
    "            img_info = {\n",
    "                \"image_path\": str(img_path),\n",
    "                \"image_name\": img_path.name,\n",
    "                \"image_stem\": img_path.stem,\n",
    "                \"dataset_name\": _get_dataset_name(img_path, datasets_path),\n",
    "                \"relative_path\": str(img_path.relative_to(datasets_path)),\n",
    "                \"md5\": md5_hash,\n",
    "            }\n",
    "\n",
    "            # –ó–∞–≥—Ä—É–∂–∞–µ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è —Ä–∞–∑–º–µ—Ä–æ–≤\n",
    "            try:\n",
    "                with Image.open(img_path) as img:\n",
    "                    img_width, img_height = img.size\n",
    "                    img_format = img.format\n",
    "                    img_mode = img.mode\n",
    "\n",
    "                img_info.update(\n",
    "                    {\n",
    "                        \"image_width\": img_width,\n",
    "                        \"image_height\": img_height,\n",
    "                        \"image_format\": img_format,\n",
    "                        \"image_mode\": img_mode,\n",
    "                        \"image_aspect_ratio\": img_width / img_height\n",
    "                        if img_height\n",
    "                        else None,\n",
    "                        \"image_total_pixels\": img_width * img_height,\n",
    "                    }\n",
    "                )\n",
    "\n",
    "                if include_image_stats:\n",
    "                    size_bytes = img_path.stat().st_size\n",
    "                    img_info.update(\n",
    "                        {\n",
    "                            \"image_size_bytes\": size_bytes,\n",
    "                            \"image_size_mb\": size_bytes / (1024 * 1024),\n",
    "                        }\n",
    "                    )\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ {img_path}: {e}\")\n",
    "                continue\n",
    "\n",
    "            # –ò—â–µ–º —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â—É—é –∞–Ω–Ω–æ—Ç–∞—Ü–∏—é\n",
    "            annotation_path = labels_dict.get(img_path.stem)\n",
    "\n",
    "            if annotation_path and annotation_path.exists():\n",
    "                img_info[\"annotation_path\"] = str(annotation_path)\n",
    "                img_info[\"has_annotation\"] = True\n",
    "\n",
    "                # –ü–∞—Ä—Å–∏–º –∞–Ω–Ω–æ—Ç–∞—Ü–∏–∏\n",
    "                try:\n",
    "                    with open(annotation_path, \"r\", encoding=\"utf-8\") as f:\n",
    "                        lines = [line.strip() for line in f.readlines() if line.strip()]\n",
    "\n",
    "                    img_info[\"objects_count\"] = len(lines)\n",
    "\n",
    "                    if len(lines) > 0:\n",
    "                        # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –∫–∞–∂–¥—ã–π –æ–±—ä–µ–∫—Ç\n",
    "                        for obj_idx, line in enumerate(lines):\n",
    "                            parts = line.split()\n",
    "                            if len(parts) >= 5:\n",
    "                                class_id = int(parts[0])\n",
    "                                x_center = float(parts[1])\n",
    "                                y_center = float(parts[2])\n",
    "                                width_norm = float(parts[3])\n",
    "                                height_norm = float(parts[4])\n",
    "\n",
    "                                # –í—ã—á–∏—Å–ª—è–µ–º –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç—ã –≤ –ø–∏–∫—Å–µ–ª—è—Ö\n",
    "                                width_pix = width_norm * img_width\n",
    "                                height_pix = height_norm * img_height\n",
    "                                x_center_pix = x_center * img_width\n",
    "                                y_center_pix = y_center * img_height\n",
    "\n",
    "                                # –ö–æ–æ—Ä–¥–∏–Ω–∞—Ç—ã —É–≥–ª–æ–≤\n",
    "                                x1 = x_center_pix - width_pix / 2\n",
    "                                y1 = y_center_pix - height_pix / 2\n",
    "                                x2 = x_center_pix + width_pix / 2\n",
    "                                y2 = y_center_pix + height_pix / 2\n",
    "\n",
    "                                # –ü–ª–æ—â–∞–¥—å –∏ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏\n",
    "                                area_pix = width_pix * height_pix\n",
    "                                area_percent = (\n",
    "                                    area_pix / (img_width * img_height)\n",
    "                                ) * 100\n",
    "                                bbox_aspect_ratio = (\n",
    "                                    width_pix / height_pix if height_pix > 0 else 0\n",
    "                                )\n",
    "\n",
    "                                # –°–æ–∑–¥–∞–µ–º –∑–∞–ø–∏—Å—å –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –æ–±—ä–µ–∫—Ç–∞\n",
    "                                obj_data = img_info.copy()\n",
    "                                obj_data.update(\n",
    "                                    {\n",
    "                                        \"object_id\": obj_idx,\n",
    "                                        \"class_id\": class_id,\n",
    "                                        \"x_center_norm\": x_center,\n",
    "                                        \"y_center_norm\": y_center,\n",
    "                                        \"width_norm\": width_norm,\n",
    "                                        \"height_norm\": height_norm,\n",
    "                                        \"x_center_pix\": x_center_pix,\n",
    "                                        \"y_center_pix\": y_center_pix,\n",
    "                                        \"width_pix\": width_pix,\n",
    "                                        \"height_pix\": height_pix,\n",
    "                                        \"x1\": x1,\n",
    "                                        \"y1\": y1,\n",
    "                                        \"x2\": x2,\n",
    "                                        \"y2\": y2,\n",
    "                                        \"bbox_area_pix\": area_pix,\n",
    "                                        \"bbox_area_percent\": area_percent,\n",
    "                                        \"bbox_aspect_ratio\": bbox_aspect_ratio,\n",
    "                                    }\n",
    "                                )\n",
    "\n",
    "                                all_data.append(obj_data)\n",
    "                    else:\n",
    "                        # –ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ –±–µ–∑ –æ–±—ä–µ–∫—Ç–æ–≤\n",
    "                        img_info[\"objects_count\"] = 0\n",
    "                        img_info[\"object_id\"] = None\n",
    "                        all_data.append(img_info)\n",
    "\n",
    "                except Exception as e:\n",
    "                    print(f\"‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–∞—Ä—Å–∏–Ω–≥–µ –∞–Ω–Ω–æ—Ç–∞—Ü–∏–∏ {annotation_path}: {e}\")\n",
    "                    img_info[\"objects_count\"] = 0\n",
    "                    img_info[\"annotation_error\"] = str(e)\n",
    "                    all_data.append(img_info)\n",
    "            else:\n",
    "                # –ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ –±–µ–∑ –∞–Ω–Ω–æ—Ç–∞—Ü–∏–∏\n",
    "                img_info[\"annotation_path\"] = None\n",
    "                img_info[\"has_annotation\"] = False\n",
    "                img_info[\"objects_count\"] = 0\n",
    "                img_info[\"object_id\"] = None\n",
    "                all_data.append(img_info)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è –û–±—â–∞—è –æ—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ {img_path}: {e}\")\n",
    "            continue\n",
    "\n",
    "    # –°–æ–∑–¥–∞–µ–º DataFrame\n",
    "    print(\"üìä –°–æ–∑–¥–∞–Ω–∏–µ DataFrame...\")\n",
    "    df = pd.DataFrame(all_data)\n",
    "\n",
    "    # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ –ø—É—Ç–∏ –∫ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—é –∏ id –æ–±—ä–µ–∫—Ç–∞\n",
    "    df = df.sort_values([\"image_path\", \"object_id\"]).reset_index(drop=True)\n",
    "\n",
    "    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ parquet\n",
    "    parquet_path = output_path\n",
    "    if not str(parquet_path).endswith(\".parquet\"):\n",
    "        parquet_path = output_path / \"dataset_info.parquet\"\n",
    "\n",
    "    print(f\"üíæ –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≤ {parquet_path}\")\n",
    "    df.to_parquet(parquet_path, index=False)\n",
    "\n",
    "    # –°–æ–∑–¥–∞–µ–º summary\n",
    "    summary = {\n",
    "        \"total_images\": len(df[\"image_path\"].unique()),\n",
    "        \"total_objects\": int(df[\"object_id\"].notna().sum()),\n",
    "        \"datasets\": list(df[\"dataset_name\"].unique()),\n",
    "        \"image_formats\": list(df[\"image_format\"].unique()),\n",
    "        \"has_annotations\": int(df[\"has_annotation\"].sum()),\n",
    "        \"images_without_annotations\": int((~df[\"has_annotation\"]).sum()),\n",
    "        \"columns\": list(df.columns),\n",
    "        \"file_size_mb\": parquet_path.stat().st_size / (1024 * 1024),\n",
    "    }\n",
    "\n",
    "    # –°–æ—Ö—Ä–∞–Ω—è–µ–º summary\n",
    "    summary_path = parquet_path.parent / f\"{parquet_path.stem}_summary.json\"\n",
    "    with open(summary_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(summary, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "    print(\"\\n‚úÖ –ì–æ—Ç–æ–≤–æ!\")\n",
    "    print(f\"üìÅ Parquet —Ñ–∞–π–ª: {parquet_path}\")\n",
    "    print(f\"üìã Summary: {summary_path}\")\n",
    "    print(\n",
    "        f\"üìä –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ: {summary['total_images']} –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π, {summary['total_objects']} –æ–±—ä–µ–∫—Ç–æ–≤\"\n",
    "    )\n",
    "    print(f\"üíæ –†–∞–∑–º–µ—Ä —Ñ–∞–π–ª–∞: {summary['file_size_mb']:.2f} MB\")\n",
    "\n",
    "    return str(parquet_path)\n",
    "\n",
    "\n",
    "def _get_dataset_name(img_path: Path, base_path: Path) -> str:\n",
    "    \"\"\"–û–ø—Ä–µ–¥–µ–ª—è–µ—Ç –Ω–∞–∑–≤–∞–Ω–∏–µ –¥–∞—Ç–∞—Å–µ—Ç–∞ –ø–æ –ø—É—Ç–∏ —Ñ–∞–π–ª–∞\"\"\"\n",
    "    try:\n",
    "        relative_path = img_path.relative_to(base_path)\n",
    "        # –ë–µ—Ä–µ–º –ø–µ—Ä–≤—É—é –ø–∞–ø–∫—É –≤ –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω–æ–º –ø—É—Ç–∏\n",
    "        parts = relative_path.parts\n",
    "        if len(parts) > 0:\n",
    "            dataset_folder = parts[0]\n",
    "            # –°–æ–∫—Ä–∞—â–∞–µ–º –¥–ª–∏–Ω–Ω—ã–µ –Ω–∞–∑–≤–∞–Ω–∏—è –¥–ª—è —É–¥–æ–±—Å—Ç–≤–∞\n",
    "            if dataset_folder == \"01_train-s1__DataSet_Human_Rescue\":\n",
    "                return \"train_s1\"\n",
    "            elif dataset_folder == \"02_second_part_DataSet_Human_Rescue\":\n",
    "                return \"train_s2\"\n",
    "            elif dataset_folder == \"03_validation__DataSet_Human_Rescue\":\n",
    "                return \"validation\"\n",
    "            else:\n",
    "                return dataset_folder\n",
    "        else:\n",
    "            return \"unknown\"\n",
    "    except Exception:\n",
    "        return \"unknown\"\n",
    "\n",
    "\n",
    "def load_dataset_info(parquet_path: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    –ó–∞–≥—Ä—É–∂–∞–µ—Ç –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –¥–∞—Ç–∞—Å–µ—Ç–µ –∏–∑ parquet —Ñ–∞–π–ª–∞\n",
    "\n",
    "    Args:\n",
    "        parquet_path: –ü—É—Ç—å –∫ parquet —Ñ–∞–π–ª—É\n",
    "\n",
    "    Returns:\n",
    "        DataFrame —Å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–µ–π –æ –¥–∞—Ç–∞—Å–µ—Ç–µ\n",
    "    \"\"\"\n",
    "    return pd.read_parquet(parquet_path)\n",
    "\n",
    "\n",
    "def get_dataset_summary(df: pd.DataFrame) -> dict:\n",
    "    \"\"\"\n",
    "    –°–æ–∑–¥–∞–µ—Ç –∫—Ä–∞—Ç–∫—É—é —Å–≤–æ–¥–∫—É –ø–æ –¥–∞—Ç–∞—Å–µ—Ç—É\n",
    "\n",
    "    Args:\n",
    "        df: DataFrame —Å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–µ–π –æ –¥–∞—Ç–∞—Å–µ—Ç–µ\n",
    "\n",
    "    Returns:\n",
    "        –°–ª–æ–≤–∞—Ä—å —Å–æ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–æ–π\n",
    "    \"\"\"\n",
    "    summary = {\n",
    "        \"total_images\": len(df[\"image_path\"].unique()),\n",
    "        \"total_objects\": int(df[\"object_id\"].notna().sum()),\n",
    "        \"avg_objects_per_image\": df.groupby(\"image_path\")[\"object_id\"].count().mean(),\n",
    "        \"datasets\": df[\"dataset_name\"].value_counts().to_dict(),\n",
    "        \"image_size_stats\": {\n",
    "            \"width\": {\n",
    "                \"mean\": df[\"image_width\"].mean(),\n",
    "                \"std\": df[\"image_width\"].std(),\n",
    "                \"min\": df[\"image_width\"].min(),\n",
    "                \"max\": df[\"image_width\"].max(),\n",
    "            },\n",
    "            \"height\": {\n",
    "                \"mean\": df[\"image_height\"].mean(),\n",
    "                \"std\": df[\"image_height\"].std(),\n",
    "                \"min\": df[\"image_height\"].min(),\n",
    "                \"max\": df[\"image_height\"].max(),\n",
    "            },\n",
    "        },\n",
    "        \"bbox_size_stats\": {\n",
    "            \"area_percent\": {\n",
    "                \"mean\": df[df[\"object_id\"].notna()][\"bbox_area_percent\"].mean(),\n",
    "                \"std\": df[df[\"object_id\"].notna()][\"bbox_area_percent\"].std(),\n",
    "                \"min\": df[df[\"object_id\"].notna()][\"bbox_area_percent\"].min(),\n",
    "                \"max\": df[df[\"object_id\"].notna()][\"bbox_area_percent\"].max(),\n",
    "            }\n",
    "        },\n",
    "    }\n",
    "\n",
    "    return summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# –°–æ–∑–¥–∞–Ω–∏–µ parquet —Ñ–∞–π–ª–∞\n",
    "parquet_file = extract_dataset_to_parquet(\n",
    "    datasets_path=\"./dataset\",  # –í–∞—à –ø—É—Ç—å –∫ –¥–∞—Ç–∞—Å–µ—Ç–∞–º\n",
    "    output_path=\"./dataset_analysis.parquet\",\n",
    "    sample_size=None,  # None = –≤—Å–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è\n",
    "    include_image_stats=True,\n",
    ")\n",
    "\n",
    "# # –ë—ã—Å—Ç—Ä–∞—è –∑–∞–≥—Ä—É–∑–∫–∞ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞\n",
    "# df = load_dataset_info(\"./dataset_analysis.parquet\")\n",
    "# summary = get_dataset_summary(df)\n",
    "\n",
    "# # –ê–Ω–∞–ª–∏–∑ –±–µ–∑ –∫–∞—Ä—Ç–∏–Ω–æ–∫!\n",
    "# outliers = df.nlargest(10, 'bbox_area_percent')\n",
    "# small_objects = df[df['bbox_area_percent'] < 0.1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5% –æ—Ç –∫–∞–∂–¥–æ–≥–æ –¥–∞—Ç–∞—Å–µ—Ç–∞\n",
    "df = pl.read_parquet(\"dataset_analysis.parquet\")\n",
    "# sampled = df.group_by('dataset_name').map_groups(lambda x: x.sample(fraction=0.05, seed=42))\n",
    "# sampled.write_csv('sample_dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sampled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# –ü—Ä–æ–≤–µ—Ä—è–µ–º, –µ—Å—Ç—å –ª–∏ –¥—É–±–ª–∏–∫–∞—Ç—ã –∏–º—ë–Ω –∫–æ–ª–æ–Ω–æ–∫\n",
    "dupe_cols = [c for c in df.columns if df.columns.count(c) > 1]\n",
    "print(\n",
    "    f\"Duplicate column names: {dupe_cols}\"\n",
    "    if dupe_cols\n",
    "    else \"All column names unique ‚úÖ\"\n",
    ")\n",
    "\n",
    "# –ü—Ä–æ—Ü–µ–Ω—Ç –ø—Ä–æ–ø—É—Å–∫–æ–≤ ¬´–∏–∑ –∫–æ—Ä–æ–±–∫–∏¬ª\n",
    "nulls = (\n",
    "    df.null_count()  # <-- Series {col: n_nulls}\n",
    "    .melt(variable_name=\"column\", value_name=\"null_cnt\")\n",
    "    .with_columns((pl.col(\"null_cnt\") / df.height * 100).round(2).alias(\"null_pct\"))\n",
    "    .filter(pl.col(\"null_cnt\") > 0)\n",
    "    .sort(\"null_cnt\", descending=True)\n",
    ")\n",
    "\n",
    "nulls.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# –ó–∞–≥—Ä—É–∂–∞–µ–º –¥–∞–Ω–Ω—ã–µ\n",
    "df = pd.read_parquet(\"dataset_analysis.parquet\")\n",
    "\n",
    "# –£–±–∏—Ä–∞–µ–º –¥—É–±–ª–∏–∫–∞—Ç—ã –ø–æ image_path\n",
    "df_img = (\n",
    "    df[[\"image_path\", \"md5\", \"dataset_name\"]]\n",
    "    .drop_duplicates(subset=[\"image_path\"])\n",
    "    .dropna(subset=[\"md5\"])\n",
    ")\n",
    "\n",
    "counts = df_img[\"md5\"].value_counts()\n",
    "\n",
    "# –ö–æ–ª-–≤–æ –≥—Ä—É–ø–ø –¥—É–±–ª–∏–∫–∞—Ç–æ–≤\n",
    "num_duplicate_hashes = int((counts > 1).sum())\n",
    "\n",
    "# –ö–æ–ª-–≤–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –≤ –≥—Ä—É–ø–ø–∞—Ö –¥—É–±–ª–∏–∫–∞—Ç–æ–≤\n",
    "images_in_duplicate_hashes = int(counts[counts > 1].sum())\n",
    "\n",
    "# –õ–∏—à–Ω–∏–µ —Ñ–∞–π–ª—ã (–≤ –∫–∞–∂–¥–æ–π –≥—Ä—É–ø–ø–µ –æ–¥–∏–Ω –æ—Ä–∏–≥–∏–Ω–∞–ª)\n",
    "redundant_images = int((counts[counts > 1] - 1).sum())\n",
    "\n",
    "print(\"–ì—Ä—É–ø–ø –¥—É–±–ª–∏–∫–∞—Ç–æ–≤ (md5 —Å >1 —Ñ–∞–π–ª–æ–º):\", num_duplicate_hashes)\n",
    "print(\"–ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –≤ –≥—Ä—É–ø–ø–∞—Ö –¥—É–±–ª–∏–∫–∞—Ç–æ–≤:\", images_in_duplicate_hashes)\n",
    "print(\"–õ–∏—à–Ω–∏—Ö —Ñ–∞–π–ª–æ–≤ (–ø–æ–≤—Ç–æ—Ä–æ–≤ —Å–≤–µ—Ä—Ö 1 –Ω–∞ –≥—Ä—É–ø–ø—É):\", redundant_images)\n",
    "\n",
    "# –¢–∞–±–ª–∏—Ü–∞ –≥—Ä—É–ø–ø –¥—É–±–ª–∏–∫–∞—Ç–æ–≤ —Å –¥–∞—Ç–∞—Å–µ—Ç–∞–º–∏\n",
    "dup_groups = (\n",
    "    df_img.groupby(\"md5\")\n",
    "    .agg(\n",
    "        count=(\"image_path\", \"count\"),\n",
    "        paths=(\"image_path\", list),\n",
    "        datasets=(\n",
    "            \"dataset_name\",\n",
    "            lambda x: sorted((x)),\n",
    "        ),  # —Å–æ–±–∏—Ä–∞–µ–º —É–Ω–∏–∫–∞–ª—å–Ω—ã–µ –¥–∞—Ç–∞—Å–µ—Ç—ã\n",
    "    )\n",
    "    .reset_index()\n",
    "    .query(\"count > 1\")\n",
    "    .sort_values(\"count\", ascending=False)\n",
    ")\n",
    "\n",
    "print(dup_groups.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dup_groups = (\n",
    "    df_img.groupby(\"md5\")\n",
    "    .agg(\n",
    "        count=(\"image_path\", \"count\"),\n",
    "        paths=(\"image_path\", list),\n",
    "        datasets=(\"dataset_name\", lambda x: dict(pd.Series(x).value_counts())),\n",
    "    )\n",
    "    .reset_index()\n",
    "    .query(\"count > 1\")\n",
    "    .sort_values(\"count\", ascending=False)\n",
    ")\n",
    "\n",
    "print(dup_groups.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import combinations\n",
    "from collections import Counter\n",
    "\n",
    "pair_counts = Counter()\n",
    "\n",
    "for ds_dict in dup_groups[\"datasets\"]:\n",
    "    datasets = list(ds_dict.keys())\n",
    "    if len(datasets) > 1:\n",
    "        for combo in combinations(sorted(datasets), 2):\n",
    "            pair_counts[combo] += 1\n",
    "\n",
    "pair_df = pd.DataFrame(\n",
    "    [(a, b, c) for (a, b), c in pair_counts.items()],\n",
    "    columns=[\"dataset_1\", \"dataset_2\", \"duplicates\"],\n",
    ").sort_values(\"duplicates\", ascending=False)\n",
    "\n",
    "print(pair_df.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== –ü–∞—Ä–∞–º–µ—Ç—Ä—ã =====\n",
    "parquet_path = \"dataset_analysis.parquet\"  # <-- –≤–∞—à parquet\n",
    "export_dir: str | None = None  # –Ω–∞–ø—Ä–∏–º–µ—Ä \"md5_reports\" –∏–ª–∏ None\n",
    "top_conflicts_preview = 20\n",
    "\n",
    "# ===== 1) –£–Ω–∏–∫–∞–ª—å–Ω—ã–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –∏ md5-–≥—Ä—É–ø–ø—ã > 1 =====\n",
    "df_img_lz = (\n",
    "    pl.scan_parquet(parquet_path)\n",
    "    .select([\"image_path\", \"md5\"])\n",
    "    .unique(subset=[\"image_path\"])  # –ø–æ –æ–¥–Ω–æ–º—É —Ä—è–¥—É –Ω–∞ —Ñ–∞–π–ª\n",
    "    .filter(pl.col(\"md5\").is_not_null() & (pl.col(\"md5\") != \"\"))\n",
    ")\n",
    "\n",
    "md5_counts = df_img_lz.group_by(\"md5\").agg(pl.len().alias(\"count\"))\n",
    "\n",
    "dup_md5_df = (\n",
    "    md5_counts.filter(pl.col(\"count\") > 1).sort(\"count\", descending=True).collect()\n",
    ")\n",
    "dup_md5_list = dup_md5_df[\"md5\"].to_list()\n",
    "\n",
    "num_duplicate_hashes = int(dup_md5_df.height)\n",
    "images_in_duplicate_hashes = int(dup_md5_df[\"count\"].sum())\n",
    "redundant_images = int((dup_md5_df[\"count\"] - 1).sum())\n",
    "\n",
    "print(\"==== MD5-–¥—É–±–ª–∏–∫–∞—Ç—ã –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π ====\")\n",
    "print(f\"MD5-–≥—Ä—É–ø–ø —Å >1 —Ñ–∞–π–ª–æ–º:              {num_duplicate_hashes}\")\n",
    "print(f\"–ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –≤ –≥—Ä—É–ø–ø–∞—Ö –¥—É–±–ª–∏–∫–∞—Ç–æ–≤:   {images_in_duplicate_hashes}\")\n",
    "print(f\"–õ–∏—à–Ω–∏—Ö —Ñ–∞–π–ª–æ–≤ (—Å–≤–µ—Ä—Ö 1 –Ω–∞ –≥—Ä—É–ø–ø—É):  {redundant_images}\")\n",
    "\n",
    "# ===== 2) –°–∏–≥–Ω–∞—Ç—É—Ä—ã –∞–Ω–Ω–æ—Ç–∞—Ü–∏–π –ø–æ –∫–∞–∂–¥–æ–º—É –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—é =====\n",
    "df_rows_lz = (\n",
    "    pl.scan_parquet(parquet_path)\n",
    "    .select(\n",
    "        [\n",
    "            \"image_path\",\n",
    "            \"md5\",\n",
    "            \"object_id\",\n",
    "            \"class_id\",\n",
    "            \"x_center_norm\",\n",
    "            \"y_center_norm\",\n",
    "            \"width_norm\",\n",
    "            \"height_norm\",\n",
    "        ]\n",
    "    )\n",
    "    .filter(pl.col(\"md5\").is_in(dup_md5_list))\n",
    ")\n",
    "\n",
    "# –§–æ—Ä–º–∏—Ä—É–µ–º —Å—Ç—Ä–æ–∫—É –æ–¥–Ω–æ–π –∞–Ω–Ω–æ—Ç–∞—Ü–∏–∏: \"class x y w h\" (–æ–∫—Ä—É–≥–ª–µ–Ω–∏–µ –¥–æ 6 –∑–Ω–∞–∫–æ–≤)\n",
    "df_rows_lz = df_rows_lz.with_columns(\n",
    "    [\n",
    "        pl.when(pl.col(\"object_id\").is_not_null())\n",
    "        .then(\n",
    "            pl.concat_str(\n",
    "                [\n",
    "                    pl.col(\"class_id\").cast(pl.Int64, strict=False).cast(pl.Utf8),\n",
    "                    pl.col(\"x_center_norm\").round(6).cast(pl.Utf8),\n",
    "                    pl.col(\"y_center_norm\").round(6).cast(pl.Utf8),\n",
    "                    pl.col(\"width_norm\").round(6).cast(pl.Utf8),\n",
    "                    pl.col(\"height_norm\").round(6).cast(pl.Utf8),\n",
    "                ],\n",
    "                separator=\" \",\n",
    "            )\n",
    "        )\n",
    "        .otherwise(pl.lit(None))\n",
    "        .alias(\"ann_row\")\n",
    "    ]\n",
    ")\n",
    "\n",
    "# –°–±–æ—Ä —Å—Ç—Ä–æ–∫ –∞–Ω–Ω–æ—Ç–∞—Ü–∏–π –≤ —Å–ø–∏—Å–æ–∫ –Ω–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ —á–µ—Ä–µ–∑ .implode()\n",
    "ann_per_image = (\n",
    "    df_rows_lz.group_by([\"image_path\", \"md5\"])\n",
    "    .agg(\n",
    "        [\n",
    "            pl.col(\"ann_row\").drop_nulls().implode().alias(\"ann_rows\"),  # -> List[str]\n",
    "        ]\n",
    "    )\n",
    "    # –®–∞–≥ 1: –ø—Ä–æ–∏–∑–≤–æ–¥–Ω—ã–µ –∫–æ–ª–æ–Ω–∫–∏ –∏–∑ ann_rows (–ù–ï —Å—Å—ã–ª–∞–µ–º—Å—è –Ω–∞ –Ω–æ–≤—ã–µ –∞–ª–∏–∞—Å—ã –≤ —ç—Ç–æ–º –∂–µ –≤—ã–∑–æ–≤–µ)\n",
    "    .with_columns(\n",
    "        [\n",
    "            pl.col(\"ann_rows\").list.sort().alias(\"ann_rows_sorted\"),\n",
    "            pl.col(\"ann_rows\").list.len().alias(\"objects_count\"),\n",
    "        ]\n",
    "    )\n",
    "    # –®–∞–≥ 2: —Ç–µ–ø–µ—Ä—å –º–æ–∂–Ω–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å —Å–æ–∑–¥–∞–Ω–Ω—ã–µ aliase'—ã\n",
    "    .with_columns(\n",
    "        [\n",
    "            (pl.col(\"objects_count\") > 0).alias(\"has_ann\"),\n",
    "            pl.when(pl.col(\"objects_count\") > 0)\n",
    "            .then(pl.col(\"ann_rows_sorted\").list.join(\"\\n\"))\n",
    "            .otherwise(pl.lit(\"\"))\n",
    "            .alias(\"ann_text\"),\n",
    "        ]\n",
    "    )\n",
    "    # –•—ç—à —Å–æ–¥–µ—Ä–∂–∏–º–æ–≥–æ –∞–Ω–Ω–æ—Ç–∞—Ü–∏–π (–∏–ª–∏ \"NOANN\", –µ—Å–ª–∏ –ø—É—Å—Ç–æ)\n",
    "    .with_columns(\n",
    "        [\n",
    "            pl.col(\"ann_text\")\n",
    "            .map_elements(\n",
    "                lambda s: hashlib.md5(s.encode(\"utf-8\")).hexdigest() if s else \"NOANN\"\n",
    "            )\n",
    "            .alias(\"ann_sig\")\n",
    "        ]\n",
    "    )\n",
    "    .select([\"image_path\", \"md5\", \"ann_sig\", \"has_ann\", \"objects_count\"])\n",
    "    .collect()\n",
    ")\n",
    "\n",
    "# ===== 3) –ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è md5-–≥—Ä—É–ø–ø –ø–æ —Å—Ç–∞—Ç—É—Å—É –∞–Ω–Ω–æ—Ç–∞—Ü–∏–π =====\n",
    "# –†–∞–∑–±–∏–≤–∫–∞ md5 √ó ann_sig -> count, paths\n",
    "per_md5_sig = (\n",
    "    ann_per_image.lazy()\n",
    "    .group_by([\"md5\", \"ann_sig\"])\n",
    "    .agg(\n",
    "        [\n",
    "            pl.len().alias(\"count\"),\n",
    "            pl.col(\"image_path\").implode().alias(\"paths\"),\n",
    "        ]\n",
    "    )\n",
    ")\n",
    "\n",
    "# md5 -> group_size, —á–∏—Å–ª–æ –≤–∞—Ä–∏–∞–Ω—Ç–æ–≤, –µ—Å—Ç—å –ª–∏ NOANN, —Å–ø–∏—Å–∫–∏ –≤–∞—Ä–∏–∞–Ω—Ç–æ–≤/—Ä–∞–∑–º–µ—Ä–æ–≤\n",
    "per_md5 = (\n",
    "    per_md5_sig.group_by(\"md5\")\n",
    "    .agg(\n",
    "        [\n",
    "            pl.sum(\"count\").alias(\"group_size\"),\n",
    "            pl.len().alias(\"num_annotation_variants\"),\n",
    "            (pl.col(\"ann_sig\") == \"NOANN\").any().alias(\"has_noann\"),\n",
    "            pl.col(\"ann_sig\").implode().alias(\"ann_sigs\"),\n",
    "            pl.col(\"count\").implode().alias(\"ann_sig_counts\"),\n",
    "        ]\n",
    "    )\n",
    "    # variants_excluding_noann = num_annotation_variants - (has_noann ? 1 : 0)\n",
    "    .with_columns(\n",
    "        [\n",
    "            pl.when(pl.col(\"has_noann\"))\n",
    "            .then(pl.col(\"num_annotation_variants\") - 1)\n",
    "            .otherwise(pl.col(\"num_annotation_variants\"))\n",
    "            .alias(\"variants_excluding_noann\")\n",
    "        ]\n",
    "    )\n",
    "    # —Ñ–∏–Ω–∞–ª—å–Ω—ã–π —Å—Ç–∞—Ç—É—Å\n",
    "    .with_columns(\n",
    "        [\n",
    "            pl.when((pl.col(\"num_annotation_variants\") == 1) & pl.col(\"has_noann\"))\n",
    "            .then(pl.lit(\"all_missing\"))\n",
    "            .when((pl.col(\"variants_excluding_noann\") == 1) & (~pl.col(\"has_noann\")))\n",
    "            .then(pl.lit(\"all_identical\"))\n",
    "            .when(pl.col(\"has_noann\") & (pl.col(\"variants_excluding_noann\") >= 1))\n",
    "            .then(pl.lit(\"partial_missing\"))\n",
    "            .otherwise(pl.lit(\"conflict\"))\n",
    "            .alias(\"ann_status\")\n",
    "        ]\n",
    "    )\n",
    "    .collect()\n",
    ")\n",
    "\n",
    "# –û—Å—Ç–∞–≤–∏–º —Ç–æ–ª—å–∫–æ —Ä–µ–∞–ª—å–Ω—ã–µ –≥—Ä—É–ø–ø—ã (>1 —Ñ–∞–π–ª–∞)\n",
    "per_md5 = per_md5.filter(pl.col(\"group_size\") > 1)\n",
    "\n",
    "# –ú–µ—Ç—Ä–∏–∫–∏ –ø–æ —Å—Ç–∞—Ç—É—Å–∞–º\n",
    "n_groups = per_md5.height\n",
    "n_all_identical = per_md5.filter(pl.col(\"ann_status\") == \"all_identical\").height\n",
    "n_all_missing = per_md5.filter(pl.col(\"ann_status\") == \"all_missing\").height\n",
    "n_partial_missing = per_md5.filter(pl.col(\"ann_status\") == \"partial_missing\").height\n",
    "n_conflict = per_md5.filter(pl.col(\"ann_status\") == \"conflict\").height\n",
    "\n",
    "print(\"\\n==== –ê–Ω–Ω–æ—Ç–∞—Ü–∏–∏ –ø–æ md5-–≥—Ä—É–ø–ø–∞–º ====\")\n",
    "print(f\"–í—Å–µ–≥–æ md5-–≥—Ä—É–ø–ø (>1 —Ñ–∞–π–ª):          {n_groups}\")\n",
    "print(f\"  –æ–¥–∏–Ω–∞–∫–æ–≤—ã–µ –∞–Ω–Ω–æ—Ç–∞—Ü–∏–∏:             {n_all_identical}\")\n",
    "print(f\"  —É –≤—Å–µ—Ö –æ—Ç—Å—É—Ç—Å—Ç–≤—É—é—Ç:               {n_all_missing}\")\n",
    "print(f\"  —á–∞—Å—Ç–∏—á–Ω–æ –æ—Ç—Å—É—Ç—Å—Ç–≤—É—é—Ç:             {n_partial_missing}\")\n",
    "print(f\"  –ö–û–ù–§–õ–ò–ö–¢ (—Ä–∞–∑–Ω—ã–µ –∞–Ω–Ω–æ—Ç–∞—Ü–∏–∏):      {n_conflict}\")\n",
    "\n",
    "# –†–∞–∑–≤—ë—Ä—Ç–∫–∞ –∫–æ–Ω—Ñ–ª–∏–∫—Ç–æ–≤ (md5 √ó ann_sig)\n",
    "per_md5_sig_df = per_md5_sig.collect()\n",
    "conflict_md5 = per_md5.filter(pl.col(\"ann_status\") == \"conflict\").select(\n",
    "    [\"md5\", \"group_size\"]\n",
    ")\n",
    "conflict_breakdown = per_md5_sig_df.join(conflict_md5, on=\"md5\", how=\"inner\").sort(\n",
    "    [\"group_size\", \"count\"], descending=[True, True]\n",
    ")\n",
    "\n",
    "print(\"\\n==== –ü—Ä–∏–º–µ—Ä –∫–æ–Ω—Ñ–ª–∏–∫—Ç–Ω—ã—Ö –≥—Ä—É–ø–ø (–ø–µ—Ä–≤—ã–µ —Å—Ç—Ä–æ–∫–∏) ====\")\n",
    "print(conflict_breakdown.head(top_conflicts_preview))\n",
    "\n",
    "# ===== 4) –î—É–±–ª–∏—Ä—É—é—â–∏–µ—Å—è –±–æ–∫—Å—ã –≤–Ω—É—Ç—Ä–∏ –æ–¥–Ω–æ–≥–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è =====\n",
    "dup_boxes = (\n",
    "    df_rows_lz.filter(pl.col(\"ann_row\").is_not_null())\n",
    "    .group_by([\"image_path\", \"ann_row\"])\n",
    "    .agg(pl.len().alias(\"count\"))\n",
    "    .filter(pl.col(\"count\") > 1)\n",
    "    .collect()\n",
    ")\n",
    "\n",
    "print(\"\\n==== –î—É–±–ª–∏—Ä—É—é—â–∏–µ—Å—è –±–æ–∫—Å—ã –≤–Ω—É—Ç—Ä–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è (–ø–µ—Ä–≤—ã–µ —Å—Ç—Ä–æ–∫–∏) ====\")\n",
    "print(dup_boxes.head(20))\n",
    "\n",
    "# ===== 5) (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ) –°–æ—Ö—Ä–∞–Ω–∏—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã =====\n",
    "if export_dir is not None:\n",
    "    out = Path(export_dir)\n",
    "    out.mkdir(parents=True, exist_ok=True)\n",
    "    dup_md5_df.write_csv(out / \"md5_duplicate_groups_counts.csv\")\n",
    "    per_md5.write_csv(out / \"md5_annotation_status.csv\")\n",
    "    conflict_breakdown.write_csv(out / \"md5_annotation_conflicts.csv\")\n",
    "    dup_boxes.write_csv(out / \"duplicate_boxes_within_image.csv\")\n",
    "    print(f\"\\n–§–∞–π–ª—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤: {out.resolve()}\")\n",
    "\n",
    "# –ù–∞ –≤—ã—Ö–æ–¥–µ –¥–æ—Å—Ç—É–ø–Ω—ã:\n",
    "# dup_md5_df         -> md5 –∏ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ñ–∞–π–ª–æ–≤ (—Ç–æ–ª—å–∫–æ –≥—Ä—É–ø–ø—ã >1)\n",
    "# ann_per_image      -> –ø–æ 1 —Ä—è–¥—É –Ω–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ: ann_sig / has_ann / objects_count\n",
    "# per_md5            -> —Å–≤–æ–¥–∫–∞ –ø–æ md5-–≥—Ä—É–ø–ø–∞–º –∏ –∏—Ö —Å—Ç–∞—Ç—É—Å\n",
    "# conflict_breakdown -> —Ä–∞–∑—Ä–µ–∑ –ø–æ –∫–æ–Ω—Ñ–ª–∏–∫—Ç–Ω—ã–º md5: (md5, ann_sig, count, paths, group_size)\n",
    "# dup_boxes          -> –ø–æ–≤—Ç–æ—Ä—è—é—â–∏–µ—Å—è —Å—Ç—Ä–æ–∫–∏ YOLO –≤–Ω—É—Ç—Ä–∏ –æ–¥–Ω–æ–≥–æ image_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obj_dist = (\n",
    "    df.group_by(\"image_stem\")\n",
    "    .agg(pl.max(\"objects_count\").alias(\"objects\"))\n",
    "    .group_by(\"objects\")\n",
    "    .count()\n",
    "    .sort(\"objects\")\n",
    ")\n",
    "print(obj_dist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) –ö–∞–¥—Ä ‚Üí people count\n",
    "obj_per_frame = df.group_by(\"image_stem\").agg(pl.max(\"objects_count\").alias(\"objects\"))\n",
    "\n",
    "# 2) –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ\n",
    "obj_dist = (obj_per_frame.group_by(\"objects\").count().sort(\"objects\")).to_pandas()\n",
    "\n",
    "# 3) –ø—Ä–µ–≤—Ä–∞—â–∞–µ–º X‚Äë–æ—Å—å –≤ –∫–∞—Ç–µ–≥–æ—Ä–∏—é\n",
    "obj_dist[\"objects\"] = obj_dist[\"objects\"].astype(str)\n",
    "\n",
    "fig = px.bar(\n",
    "    obj_dist,\n",
    "    x=\"objects\",\n",
    "    y=\"count\",\n",
    "    title=\"–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ objects_count (—É–Ω–∏–∫–∞–ª—å–Ω—ã–µ –∫–∞–¥—Ä—ã)\",\n",
    "    labels={\"objects\": \"objects_count\", \"count\": \"–∫–∞–¥—Ä–æ–≤\"},\n",
    "    text_auto=True,  # –ø–æ–¥–ø–∏—Å–∏ –Ω–∞–¥ –±–∞—Ä–∞–º–∏ (–æ–ø—Ü.)\n",
    ")\n",
    "fig.update_layout(\n",
    "    xaxis_title=\"objects_count\", yaxis_title=\"–∫–∞–¥—Ä–æ–≤\", bargap=0.1\n",
    ")  # —á—É—Ç—å —É–∂–µ –∑–∞–∑–æ—Ä—ã\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# –ü—Ä–µ–¥–ø–æ–ª–æ–∂–∏–º, —Å—Ç–æ–ª–±—Ü—ã image_width / image_height —É–∂–µ –ø–æ—Å—á–∏—Ç–∞–Ω—ã.\n",
    "# –ï—Å–ª–∏ –∏—Ö –Ω–µ—Ç, –º–æ–∂–Ω–æ –ø—Ä–æ—á–∏—Ç–∞—Ç—å —Ä–∞–∑–º–µ—Ä—ã –ø—Ä—è–º–æ –∏–∑ —Ñ–∞–π–ª–æ–≤ (–º–µ–¥–ª–µ–Ω–Ω–µ–µ).\n",
    "size_stats = (\n",
    "    df.group_by(\"image_stem\")\n",
    "    .first()\n",
    "    .select(\"image_stem\", \"image_width\", \"image_height\")\n",
    "    .unique()\n",
    "    .select(\n",
    "        [\n",
    "            pl.mean(\"image_width\").alias(\"avg_w\"),\n",
    "            pl.mean(\"image_height\").alias(\"avg_h\"),\n",
    "            pl.min(\"image_width\").alias(\"min_w\"),\n",
    "            pl.max(\"image_width\").alias(\"max_w\"),\n",
    "            pl.min(\"image_height\").alias(\"min_h\"),\n",
    "            pl.max(\"image_height\").alias(\"max_h\"),\n",
    "        ]\n",
    "    )\n",
    ")\n",
    "print(size_stats)\n",
    "\n",
    "# –°–æ–æ—Ç–Ω–æ—à–µ–Ω–∏–µ —Å—Ç–æ—Ä–æ–Ω\n",
    "ratio_df = df.select(\n",
    "    \"image_stem\", (pl.col(\"image_width\") / pl.col(\"image_height\")).alias(\"aspect\")\n",
    ").unique()\n",
    "\n",
    "px.histogram(\n",
    "    ratio_df.to_pandas(), x=\"aspect\", nbins=40, title=\"–°–æ–æ—Ç–Ω–æ—à–µ–Ω–∏–µ —Å—Ç–æ—Ä–æ–Ω –∫–∞–¥—Ä–æ–≤\"\n",
    ").show()\n",
    "\n",
    "# –≠–∫—Å—Ç—Ä–µ–º–∞–ª—å–Ω—ã–µ —Ñ–∞–π–ª—ã\n",
    "tiny = df.filter((pl.col(\"image_width\") < 320) | (pl.col(\"image_height\") < 320))\n",
    "huge = df.filter((pl.col(\"image_width\") > 4000) | (pl.col(\"image_height\") > 4000))\n",
    "print(f\"Tiny frames: {tiny.select('image_stem').n_unique()}\")\n",
    "print(f\"Huge frames: {huge.select('image_stem').n_unique()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bbox_x, bbox_y = \"x1\", \"y1\"  # –ª–µ–≤—ã–π‚Äë–≤–µ—Ä—Ö–Ω–∏–π —É–≥–æ–ª\n",
    "bbox_w, bbox_h = \"width_pix\", \"height_pix\"\n",
    "img_w, img_h = \"image_width\", \"image_height\"\n",
    "\n",
    "bbox = df.filter(pl.col(\"objects_count\") > 0).with_columns(\n",
    "    [\n",
    "        (pl.col(bbox_w) * pl.col(bbox_h)).alias(\"bbox_area_pix\"),\n",
    "        (\n",
    "            (pl.col(bbox_w) * pl.col(bbox_h)) / (pl.col(img_w) * pl.col(img_h)) * 100\n",
    "        ).alias(\"bbox_pct\"),\n",
    "        (pl.col(bbox_w) / pl.col(bbox_h)).alias(\"bbox_aspect\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "px.histogram(\n",
    "    bbox.to_pandas(), x=\"bbox_pct\", nbins=50, title=\"BBox area (% –∫–∞–¥—Ä–∞)\"\n",
    ").show()\n",
    "px.histogram(\n",
    "    bbox.to_pandas(), x=\"bbox_aspect\", nbins=50, title=\"BBox aspect ratio\"\n",
    ").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Jupyter cell 6 ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "import polars as pl\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def safe_sample(df_sub, n=5, seed=42):\n",
    "    k = min(df_sub.height, n)\n",
    "    return df_sub.sample(k, seed=seed) if k > 0 else pl.DataFrame()\n",
    "\n",
    "\n",
    "suspect_big = bbox.filter(pl.col(\"bbox_pct\") > 70)\n",
    "suspect_small = bbox.filter(pl.col(\"bbox_pct\") < 1)\n",
    "\n",
    "print(\"Big bbox frames:\", suspect_big.select(\"image_stem\").n_unique())\n",
    "print(\"Small bbox frames:\", suspect_small.select(\"image_stem\").n_unique())\n",
    "\n",
    "examples_df = pl.concat([safe_sample(suspect_big), safe_sample(suspect_small)])\n",
    "\n",
    "if examples_df.is_empty():\n",
    "    print(\"‚ö†Ô∏è  –ù–µ—Ç –ø—Ä–∏–º–µ—Ä–æ–≤, —É–¥–æ–≤–ª–µ—Ç–≤–æ—Ä—è—é—â–∏—Ö –∫—Ä–∏—Ç–µ—Ä–∏—è–º.\")\n",
    "else:\n",
    "    thumbs = []\n",
    "    for rec in examples_df.iter_rows(named=True):\n",
    "        img = Image.open(rec[\"image_path\"]).convert(\"RGB\")\n",
    "\n",
    "        # –∫–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç —Ä–µ—Å–∞–π–∑–∞ –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω–æ –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω–æ–≥–æ –∫–∞–¥—Ä–∞\n",
    "        scale = 640 / max(rec[\"image_width\"], rec[\"image_height\"])\n",
    "        if scale < 1.0:  # —Ç–æ–ª—å–∫–æ —É–º–µ–Ω—å—à–∞–µ–º\n",
    "            new_size = (\n",
    "                int(rec[\"image_width\"] * scale),\n",
    "                int(rec[\"image_height\"] * scale),\n",
    "            )\n",
    "            img = img.resize(new_size, Image.LANCZOS)\n",
    "\n",
    "        # –ø–µ—Ä–µ—Å—á—ë—Ç bbox\n",
    "        x = rec[bbox_x] * scale\n",
    "        y = rec[bbox_y] * scale\n",
    "        w = rec[bbox_w] * scale\n",
    "        h = rec[bbox_h] * scale\n",
    "\n",
    "        draw = ImageDraw.Draw(img)\n",
    "        draw.rectangle([x, y, x + w, y + h], outline=\"red\", width=6)\n",
    "        thumbs.append(img)\n",
    "\n",
    "    # --- –≤—ã–≤–æ–¥ —Å–µ—Ç–∫–æ–π ---\n",
    "    cols = 3\n",
    "    rows = int(np.ceil(len(thumbs) / cols))\n",
    "    fig, axes = plt.subplots(rows, cols, figsize=(cols * 4, rows * 4))\n",
    "\n",
    "    for ax, im in zip(np.asarray(axes).ravel(), thumbs):\n",
    "        ax.imshow(im)\n",
    "        ax.axis(\"off\")\n",
    "\n",
    "    for ax in np.asarray(axes).ravel()[len(thumbs) :]:\n",
    "        ax.axis(\"off\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TARGET_IMG = \"example_frame_00123\"  # ‚Üê –ø–æ—Å—Ç–∞–≤—å—Ç–µ –Ω—É–∂–Ω—ã–π image_stem\n",
    "\n",
    "boxes = df.filter(pl.col(\"image_stem\") == TARGET_IMG)\n",
    "\n",
    "if boxes.is_empty():\n",
    "    print(\"üôÅ –¢–∞–∫–æ–π –∫–∞—Ä—Ç–∏–Ω–∫–∏ –Ω–µ—Ç.\")\n",
    "else:\n",
    "    img_path = boxes.select(\"image_path\").item(0)\n",
    "    img = Image.open(img_path).convert(\"RGB\")\n",
    "    drw = ImageDraw.Draw(img)\n",
    "\n",
    "    for row in boxes.iter_rows(named=True):\n",
    "        drw.rectangle(\n",
    "            [\n",
    "                row[bbox_x],\n",
    "                row[bbox_y],\n",
    "                row[bbox_x] + row[bbox_w],\n",
    "                row[bbox_y] + row[bbox_h],\n",
    "            ],\n",
    "            outline=\"orange\",\n",
    "            width=3,\n",
    "        )\n",
    "\n",
    "    display(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Jupyter cell 8 ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "heat = df.select([\"x_center_norm\", \"y_center_norm\"])\n",
    "fig = px.density_heatmap(\n",
    "    heat.to_pandas(),\n",
    "    x=\"x_center_norm\",\n",
    "    y=\"y_center_norm\",\n",
    "    nbinsx=50,\n",
    "    nbinsy=50,\n",
    "    title=\"–¢–µ–ø–ª–æ–≤–∞—è –∫–∞—Ä—Ç–∞ —Ü–µ–Ω—Ç—Ä–æ–≤ bbox (–Ω–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç—ã)\",\n",
    ")\n",
    "fig.update_yaxes(autorange=\"reversed\")  # —á—Ç–æ–±—ã (0,0) –±—ã–ª–æ –≤ –ª–µ–≤–æ–º‚Äë–≤–µ—Ä—Ö–Ω–µ–º\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Jupyter cell 9 ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "corr = df.group_by(\"image_stem\").agg(\n",
    "    [\n",
    "        pl.mean(\"bbox_area_percent\").alias(\"avg_bbox_pct\"),\n",
    "        pl.max(\"objects_count\").alias(\"objects\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "fig = px.scatter(\n",
    "    corr.to_pandas(),\n",
    "    x=\"objects\",\n",
    "    y=\"avg_bbox_pct\",\n",
    "    trendline=\"ols\",\n",
    "    title=\"–°—Ä–µ–¥–Ω—è—è –ø–ª–æ—â–∞–¥—å bbox (%) vs —á–∏—Å–ª–æ –ª—é–¥–µ–π\",\n",
    ")\n",
    "fig.update_traces(marker=dict(size=6))\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Jupyter cell 10 ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "dup_boxes = (\n",
    "    df.filter(pl.col(\"objects_count\") > 1)\n",
    "    .group_by([\"image_stem\", \"x1\", \"y1\", \"x2\", \"y2\"])\n",
    "    .count()\n",
    "    .filter(pl.col(\"count\") > 1)\n",
    ")\n",
    "\n",
    "print(\"–ü–æ–ª–Ω—ã—Ö –¥—É–±–ª–µ–π bbox:\", dup_boxes.shape[0])\n",
    "dup_boxes.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) –ï—Å–ª–∏ –Ω–µ—Ç bbox_area_percent, –ø–µ—Ä–µ—Å—á–∏—Ç—ã–≤–∞–µ–º\n",
    "if \"bbox_area_percent\" not in df.columns:\n",
    "    df = df.with_columns(\n",
    "        (\n",
    "            (pl.col(\"width_pix\") * pl.col(\"height_pix\"))\n",
    "            / (pl.col(\"image_width\") * pl.col(\"image_height\"))\n",
    "            * 100\n",
    "        ).alias(\"bbox_area_percent\")\n",
    "    )\n",
    "\n",
    "# 2) –î–æ–±–∞–≤–ª—è–µ–º —Ñ–ª–∞–≥–∏ —Ç–æ–ª—å–∫–æ –∫ —Å—Ç—Ä–æ–∫–∞–º —Å –æ–±—ä–µ–∫—Ç–∞–º–∏\n",
    "df = df.with_columns(\n",
    "    [\n",
    "        pl.when(pl.col(\"objects_count\") > 0)  # —Ñ–∏–ª—å—Ç—Ä —Ä–∞–º–æ–∫\n",
    "        .then((pl.col(\"bbox_area_percent\") > 50) | (pl.col(\"bbox_area_percent\") < 0.25))\n",
    "        .otherwise(False)\n",
    "        .alias(\"bbox_outlier\"),\n",
    "        pl.when(pl.col(\"objects_count\") > 0)\n",
    "        .then(\n",
    "            (pl.col(\"x1\") < 0)\n",
    "            | (pl.col(\"y1\") < 0)\n",
    "            | (pl.col(\"x2\") > pl.col(\"image_width\"))\n",
    "            | (pl.col(\"y2\") > pl.col(\"image_height\"))\n",
    "        )\n",
    "        .otherwise(False)\n",
    "        .alias(\"bbox_oob\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# 3) –ë—ã—Å—Ç—Ä—ã–π –ø—Ä–æ—Å–º–æ—Ç—Ä, —Å–∫–æ–ª—å–∫–æ –Ω–∞—à–ª–∏\n",
    "summary = df.select(\n",
    "    [\n",
    "        pl.col(\"bbox_outlier\").sum().alias(\"outliers\"),\n",
    "        pl.col(\"bbox_oob\").sum().alias(\"out_of_bounds\"),\n",
    "    ]\n",
    ")\n",
    "print(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dirty_frames = (\n",
    "    df.filter(pl.col(\"bbox_outlier\") | pl.col(\"bbox_oob\")).select(\"image_stem\").unique()\n",
    ")\n",
    "\n",
    "print(f\"–ù–∞–π–¥–µ–Ω–æ {dirty_frames.height} –∫–∞–¥—Ä–æ–≤ —Å –ø–æ—Ç–µ–Ω—Ü–∏–∞–ª—å–Ω—ã–º–∏ –ø—Ä–æ–±–ª–µ–º–∞–º–∏.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q01 = (\n",
    "    df.filter(pl.col(\"objects_count\") > 0)\n",
    "    .select(pl.col(\"bbox_area_percent\").quantile(0.01))\n",
    "    .item(0, 0)  # <‚îÄ row=0, col=0\n",
    ")\n",
    "\n",
    "# 2) –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞—Ç—å –≤ Series –∏ –≤–∑—è—Ç—å 1‚Äë–π —ç–ª–µ–º–µ–Ω—Ç\n",
    "q99 = (\n",
    "    df.filter(pl.col(\"objects_count\") > 0)\n",
    "    .select(pl.col(\"bbox_area_percent\").quantile(0.99))\n",
    "    .to_series()[0]  # <‚îÄ Series ‚Üí —Å–∫–∞–ª—è—Ä\n",
    ")\n",
    "\n",
    "print(f\"1‚Äë–π –ø—Ä–æ—Ü–µ–Ω—Ç–∏–ª—å:  {q01:.4f}‚ÄØ%\")\n",
    "print(f\"99‚Äë–π –ø—Ä–æ—Ü–µ–Ω—Ç–∏–ª—å: {q99:.4f}‚ÄØ%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Jupyter cell ‚Äî –ø–µ—Ä–µ—Å—á—ë—Ç —Ñ–ª–∞–≥–æ–≤ –ø–æ –Ω–æ–≤—ã–º –ø–æ—Ä–æ–≥–∞–º\n",
    "TINY_PCT = 0.01  # 0.01‚ÄØ%\n",
    "HUGE_PCT = 1.0  # 1.0‚ÄØ%\n",
    "\n",
    "df = df.with_columns(\n",
    "    [\n",
    "        # tiny/huge —Ç–æ–ª—å–∫–æ –¥–ª—è —Å—Ç—Ä–æ–∫ —Å bbox\n",
    "        pl.when(pl.col(\"objects_count\") > 0)\n",
    "        .then(\n",
    "            (pl.col(\"bbox_area_percent\") < TINY_PCT)\n",
    "            | (pl.col(\"bbox_area_percent\") > HUGE_PCT)\n",
    "        )\n",
    "        .otherwise(False)\n",
    "        .alias(\"bbox_outlier\"),\n",
    "        pl.when(pl.col(\"objects_count\") > 0)\n",
    "        .then(\n",
    "            (pl.col(\"x1\") < 0)\n",
    "            | (pl.col(\"y1\") < 0)\n",
    "            | (pl.col(\"x2\") > pl.col(\"image_width\"))\n",
    "            | (pl.col(\"y2\") > pl.col(\"image_height\"))\n",
    "        )\n",
    "        .otherwise(False)\n",
    "        .alias(\"bbox_oob\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# –°–≤–æ–¥–∫–∞\n",
    "summary = df.select(\n",
    "    [\n",
    "        pl.col(\"bbox_outlier\").sum().alias(\"outliers\"),\n",
    "        pl.col(\"bbox_oob\").sum().alias(\"out_of_bounds\"),\n",
    "    ]\n",
    ")\n",
    "print(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ipywidgets as widgets\n",
    "from IPython.display import clear_output\n",
    "\n",
    "\n",
    "# ====== –ø–∞—Ä–∞–º–µ—Ç—Ä—ã ======\n",
    "ID_COL = \"image_stem\"  # ‚Üê –º–æ–∂–Ω–æ —Å–º–µ–Ω–∏—Ç—å –Ω–∞ \"image_name\" –∏ —Ç.–ø.\n",
    "page_size = 10  # –ø–æ —Å–∫–æ–ª—å–∫–æ –∫–∞–¥—Ä–æ–≤ –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü—É\n",
    "max_side = 640  # –º–∞–∫—Å. —Ä–∞–∑–º–µ—Ä –¥–ª–∏–Ω–Ω–æ–π —Å—Ç–æ—Ä–æ–Ω—ã –º–∏–Ω–∏–∞—Ç—é—Ä—ã\n",
    "\n",
    "# ---------- –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∞ —Å–ø–∏—Å–∫–∞ –ø—Ä–æ–±–ª–µ–º–Ω—ã—Ö –∫–∞–¥—Ä–æ–≤ ----------\n",
    "flagged_df = (\n",
    "    df.filter(pl.col(\"bbox_outlier\") | pl.col(\"bbox_oob\")).select(ID_COL).unique()\n",
    ")\n",
    "flagged_list = flagged_df.to_series().to_list()\n",
    "total_frames = len(flagged_list)\n",
    "\n",
    "# —Å–ª–æ–≤–∞—Ä—å: id -> –ø–æ—Ä—è–¥–∫–æ–≤—ã–π –Ω–æ–º–µ—Ä (–¥–ª—è –ø–æ–¥–ø–∏—Å–∏ ‚Ññ)\n",
    "idx_map = {idv: i for i, idv in enumerate(flagged_list)}\n",
    "\n",
    "\n",
    "def draw_page(start_idx):\n",
    "    clear_output(wait=True)\n",
    "    subset = flagged_list[start_idx : start_idx + page_size]\n",
    "    if not subset:\n",
    "        print(\"üì≠  –ö–∞–¥—Ä–æ–≤ –±–æ–ª—å—à–µ –Ω–µ—Ç.\")\n",
    "        return start_idx\n",
    "\n",
    "    # –ü–æ–¥—Ç—è–≥–∏–≤–∞–µ–º —Å—Ç—Ä–æ–∫–∏ –ø–æ –∫–∞–¥—Ä–∞–º\n",
    "    view_df = df.filter(pl.col(ID_COL).is_in(subset))\n",
    "\n",
    "    thumbs = []\n",
    "    labels = []\n",
    "    for stem in subset:\n",
    "        rows = view_df.filter(pl.col(ID_COL) == stem)\n",
    "        rec0 = rows.row(0, named=True)\n",
    "        img = Image.open(rec0[\"image_path\"]).convert(\"RGB\")\n",
    "\n",
    "        # –º–∞—Å—à—Ç–∞–±\n",
    "        scale = max_side / max(rec0[\"image_width\"], rec0[\"image_height\"])\n",
    "        if scale < 1.0:\n",
    "            img = img.resize(\n",
    "                (int(rec0[\"image_width\"] * scale), int(rec0[\"image_height\"] * scale)),\n",
    "                Image.LANCZOS,\n",
    "            )\n",
    "\n",
    "        # —Ä–∞–º–∫–∏\n",
    "        drw = ImageDraw.Draw(img)\n",
    "        for r in rows.iter_rows(named=True):\n",
    "            x1 = r[\"x1\"] * scale\n",
    "            y1 = r[\"y1\"] * scale\n",
    "            x2 = r[\"x2\"] * scale\n",
    "            y2 = r[\"y2\"] * scale\n",
    "            color = \"red\" if r[\"bbox_oob\"] else (144, 238, 144)\n",
    "            drw.rectangle([x1, y1, x2, y2], outline=color, width=7)\n",
    "\n",
    "        thumbs.append(img)\n",
    "        labels.append(f\"#{idx_map[stem]} | {stem}\")  # –ø–æ–¥–ø–∏—Å—å\n",
    "\n",
    "    # --- —Ä–∏—Å—É–µ–º —Å–µ—Ç–∫—É ---\n",
    "    cols = 5\n",
    "    rows = int(np.ceil(len(thumbs) / cols))\n",
    "    fig, axes = plt.subplots(rows, cols, figsize=(cols * 3.2, rows * 3.5))\n",
    "\n",
    "    flat_axes = np.asarray(axes).ravel() if hasattr(axes, \"ravel\") else [axes]\n",
    "    for ax, im, lab in zip(flat_axes, thumbs, labels):\n",
    "        ax.imshow(im)\n",
    "        ax.set_title(lab, fontsize=8)\n",
    "        ax.axis(\"off\")\n",
    "\n",
    "    for ax in flat_axes[len(thumbs) :]:\n",
    "        ax.axis(\"off\")\n",
    "\n",
    "    plt.suptitle(\n",
    "        f\"Flagged frames {start_idx}‚Äì{start_idx + len(thumbs) - 1} / {total_frames}\",\n",
    "        fontsize=14,\n",
    "    )\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    return start_idx\n",
    "\n",
    "\n",
    "# ---------- –≤–∏–¥–∂–µ—Ç—ã ----------\n",
    "idx = 0\n",
    "\n",
    "prev_btn = widgets.Button(description=\"‚Üê Prev\", layout=widgets.Layout(width=\"80px\"))\n",
    "next_btn = widgets.Button(description=\"Next ‚Üí\", layout=widgets.Layout(width=\"80px\"))\n",
    "out = widgets.Output()\n",
    "\n",
    "\n",
    "def on_prev(_):\n",
    "    global idx\n",
    "    idx = max(idx - page_size, 0)\n",
    "    with out:\n",
    "        idx = draw_page(idx)\n",
    "\n",
    "\n",
    "def on_next(_):\n",
    "    global idx\n",
    "    idx = min(idx + page_size, (total_frames - 1) // page_size * page_size)\n",
    "    with out:\n",
    "        idx = draw_page(idx)\n",
    "\n",
    "\n",
    "prev_btn.on_click(on_prev)\n",
    "next_btn.on_click(on_next)\n",
    "\n",
    "display(widgets.HBox([prev_btn, next_btn]))\n",
    "with out:\n",
    "    idx = draw_page(idx)  # initial page\n",
    "display(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frame_stats = (\n",
    "    df.group_by(\"image_stem\")\n",
    "    .agg(\n",
    "        [\n",
    "            pl.max(\"objects_count\").alias(\"objects\"),\n",
    "            pl.first(\"image_total_pixels\").alias(\"pixels\"),\n",
    "        ]\n",
    "    )\n",
    "    .with_columns((pl.col(\"objects\") / (pl.col(\"pixels\") / 1_000_000)).alias(\"density\"))\n",
    ")\n",
    "px.histogram(\n",
    "    frame_stats.to_pandas(),\n",
    "    x=\"density\",\n",
    "    nbins=50,\n",
    "    title=\"–ü–ª–æ—Ç–Ω–æ—Å—Ç—å –ª—é–¥–µ–π (–æ–±—ä–µ–∫—Ç–æ–≤ –Ω–∞ 1‚ÄØ–ú–ø)\",\n",
    ").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install ensemble-boxes ipywidgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
